{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c75a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from constants import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeab63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "CLICKS_DIR = DATA_DIR / \"unzip_clicks\"\n",
    "WORK_DIR = DATA_DIR / \"workbase\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c470e2",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08dbc2",
   "metadata": {},
   "source": [
    "pour la phase de modélisation, si l'on souhaite construire un profil utilisateur, on peut utilisateur plusieurs approches :\n",
    "- [ ] Intégrer le clic ranking comme une note\n",
    "- [ ] Prendre en compte comme catégorie : la catégorie de l'article, le catégorie des labels KMEANS\n",
    "\n",
    "\n",
    "Pour la baseline, on pourra tester :\n",
    "- [ ] prédire par article popularity, pop weighted by recency & by squared root recency\n",
    "\n",
    "\n",
    "Pour la partie content base, on pourra tester :\n",
    "- [ ] Cosine similarity sur les embeddings des articles\n",
    "- [ ] Cosine similarity sur les embeddings des articles pondérés par le user profile\n",
    "- [ ] Average des embeddings du jeu de train ou last embedding du jeu de train\n",
    "\n",
    "\n",
    "Concernant les métriques, on pourra tester :\n",
    "- [ ] Le Recall@5, l'Acc@5, F1@5, le NDCG@5, le MAP@5\n",
    "\n",
    "\n",
    "Si on retient l'approche content based on envisagera de précalculer les recommandations dans l'API pour éviter de recalculer les embeddings à chaque fois.\n",
    "\n",
    "\n",
    "Stratégie à tester pour la partie collaborative :\n",
    "- [ ] ALS\n",
    "- [ ] 2-tower model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72feef05",
   "metadata": {},
   "source": [
    "Limitations\n",
    "\n",
    "\n",
    "It is important to note that the primary limitation of Precision and Recall @ k\n",
    " is that they focus solely on whether the items in the top k\n",
    " positions are relevant, without considering the order of these items within those k\n",
    " positions. These metrics thus do not measure the ranking quality of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d29230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the split date for training and testing\n",
    "SPLIT_DATE = pd.to_datetime(\"2017-10-10\")\n",
    "# Load the train & test splits with the test_df_09.pickle and test_df_10.pickle\n",
    "train_df = (\n",
    "    pd.read_pickle(WORK_DIR / \"train_filtered_10.pickle\")\n",
    "    .sort_values(\"click_timestamp\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    "    .astype(\n",
    "        {\n",
    "            \"article_id\": \"int32\",\n",
    "            \"category_id\": \"int32\",\n",
    "            \"publisher_id\": \"int32\",\n",
    "            \"words_count\": \"int32\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "test_df = (\n",
    "    pd.read_pickle(WORK_DIR / \"test_filtered_10.pickle\")\n",
    "    .sort_values(\"click_timestamp\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    "    .astype(\n",
    "        {\n",
    "            \"article_id\": \"int32\",\n",
    "            \"category_id\": \"int32\",\n",
    "            \"publisher_id\": \"int32\",\n",
    "            \"words_count\": \"int32\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "traintest_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "train = train_df.groupby(\"user_id\")[\"click_article_id\"].agg(list).sort_index()\n",
    "test = test_df.groupby(\"user_id\")[\"click_article_id\"].agg(list).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adab1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_click_pos(df, alpha=0.5):\n",
    "    avg_click_pos = (\n",
    "        df.groupby(\"article_id\")[\"click_ranking\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"click_ranking\": \"avg_click_ranking\"})\n",
    "    )\n",
    "    avg_click_pos[\"ranking_weight\"] = 1 / (avg_click_pos[\"avg_click_ranking\"] + 1)\n",
    "    avg_click_pos[\"ranking_exp_weight\"] = np.exp(\n",
    "        -alpha * avg_click_pos[\"avg_click_ranking\"]\n",
    "    )\n",
    "    return avg_click_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ae03a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "avg_click_ranking",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ranking_weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ranking_exp_weight",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "bddfba9e-cfca-478f-8f73-09f752a31091",
       "rows": [
        [
         "0",
         "2022",
         "4.0",
         "0.2",
         "0.1353352832366127"
        ],
        [
         "1",
         "2075",
         "5.266666666666667",
         "0.1595744680851064",
         "0.07183860068931329"
        ],
        [
         "2",
         "2136",
         "5.333333333333333",
         "0.15789473684210528",
         "0.06948345122280154"
        ],
        [
         "3",
         "2231",
         "2.75",
         "0.26666666666666666",
         "0.25283959580474646"
        ],
        [
         "4",
         "2303",
         "3.2666666666666666",
         "0.234375",
         "0.19527756283568573"
        ],
        [
         "5",
         "2485",
         "5.0",
         "0.16666666666666666",
         "0.0820849986238988"
        ],
        [
         "6",
         "2626",
         "6.8",
         "0.12820512820512822",
         "0.03337326996032608"
        ],
        [
         "7",
         "2647",
         "3.6056338028169015",
         "0.2171253822629969",
         "0.16483391275302836"
        ],
        [
         "8",
         "2764",
         "3.5238095238095237",
         "0.22105263157894736",
         "0.17171747122199255"
        ],
        [
         "9",
         "2907",
         "7.333333333333333",
         "0.12000000000000002",
         "0.025561533206507402"
        ],
        [
         "10",
         "3091",
         "4.565217391304348",
         "0.1796875",
         "0.10201772608474727"
        ],
        [
         "11",
         "3106",
         "6.333333333333333",
         "0.13636363636363638",
         "0.042143843509276406"
        ],
        [
         "12",
         "3148",
         "3.857142857142857",
         "0.20588235294117646",
         "0.1453557012338466"
        ],
        [
         "13",
         "3232",
         "3.5",
         "0.2222222222222222",
         "0.17377394345044514"
        ],
        [
         "14",
         "3240",
         "4.6",
         "0.17857142857142858",
         "0.10025884372280375"
        ],
        [
         "15",
         "3241",
         "4.833333333333333",
         "0.17142857142857143",
         "0.08921851740926011"
        ],
        [
         "16",
         "3246",
         "5.625",
         "0.1509433962264151",
         "0.060054667895307945"
        ],
        [
         "17",
         "3310",
         "4.6",
         "0.17857142857142858",
         "0.10025884372280375"
        ],
        [
         "18",
         "3394",
         "3.0",
         "0.25",
         "0.22313016014842982"
        ],
        [
         "19",
         "3434",
         "4.0",
         "0.2",
         "0.1353352832366127"
        ],
        [
         "20",
         "3467",
         "5.25",
         "0.16",
         "0.07243975703425146"
        ],
        [
         "21",
         "3712",
         "4.176470588235294",
         "0.19318181818181818",
         "0.12390559993936892"
        ],
        [
         "22",
         "3808",
         "7.5",
         "0.11764705882352941",
         "0.023517745856009107"
        ],
        [
         "23",
         "4374",
         "3.0625",
         "0.24615384615384617",
         "0.2162651668298873"
        ],
        [
         "24",
         "4400",
         "3.3333333333333335",
         "0.23076923076923073",
         "0.18887560283756183"
        ],
        [
         "25",
         "4528",
         "4.0",
         "0.2",
         "0.1353352832366127"
        ],
        [
         "26",
         "4578",
         "6.0",
         "0.14285714285714285",
         "0.049787068367863944"
        ],
        [
         "27",
         "4639",
         "2.727272727272727",
         "0.2682926829268293",
         "0.25572915991310063"
        ],
        [
         "28",
         "4789",
         "4.375",
         "0.18604651162790697",
         "0.11219689052034373"
        ],
        [
         "29",
         "4867",
         "5.266666666666667",
         "0.1595744680851064",
         "0.07183860068931329"
        ],
        [
         "30",
         "4907",
         "4.233333333333333",
         "0.19108280254777069",
         "0.12043240152376271"
        ],
        [
         "31",
         "4967",
         "4.666666666666667",
         "0.1764705882352941",
         "0.09697196786440505"
        ],
        [
         "32",
         "5033",
         "5.142857142857143",
         "0.1627906976744186",
         "0.07642628699076807"
        ],
        [
         "33",
         "5137",
         "5.0",
         "0.16666666666666666",
         "0.0820849986238988"
        ],
        [
         "34",
         "5254",
         "5.0",
         "0.16666666666666666",
         "0.0820849986238988"
        ],
        [
         "35",
         "5258",
         "7.0",
         "0.125",
         "0.0301973834223185"
        ],
        [
         "36",
         "5272",
         "4.75",
         "0.17391304347826086",
         "0.09301448921066349"
        ],
        [
         "37",
         "5278",
         "4.222222222222222",
         "0.19148936170212766",
         "0.12110333239232973"
        ],
        [
         "38",
         "5314",
         "2.357142857142857",
         "0.2978723404255319",
         "0.30771802192680026"
        ],
        [
         "39",
         "5341",
         "3.2857142857142856",
         "0.23333333333333334",
         "0.19342660460039254"
        ],
        [
         "40",
         "5353",
         "2.422222222222222",
         "0.29220779220779225",
         "0.29786613312364846"
        ],
        [
         "41",
         "5366",
         "1.851063829787234",
         "0.35074626865671643",
         "0.3963205541934571"
        ],
        [
         "42",
         "5387",
         "2.1714285714285713",
         "0.3153153153153153",
         "0.337660513655132"
        ],
        [
         "43",
         "5408",
         "2.3275862068965516",
         "0.3005181347150259",
         "0.31229934768244116"
        ],
        [
         "44",
         "5457",
         "4.8",
         "0.1724137931034483",
         "0.09071795328941251"
        ],
        [
         "45",
         "5559",
         "6.0",
         "0.14285714285714285",
         "0.049787068367863944"
        ],
        [
         "46",
         "7694",
         "4.081632653061225",
         "0.19678714859437751",
         "0.12992260830505947"
        ],
        [
         "47",
         "7744",
         "4.387096774193548",
         "0.18562874251497008",
         "0.11152032841273718"
        ],
        [
         "48",
         "7863",
         "2.0",
         "0.3333333333333333",
         "0.36787944117144233"
        ],
        [
         "49",
         "7902",
         "3.4444444444444446",
         "0.22499999999999998",
         "0.17866866494966513"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 3243
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>avg_click_ranking</th>\n",
       "      <th>ranking_weight</th>\n",
       "      <th>ranking_exp_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.135335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2075</td>\n",
       "      <td>5.266667</td>\n",
       "      <td>0.159574</td>\n",
       "      <td>0.071839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2136</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.069483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2231</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.252840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2303</td>\n",
       "      <td>3.266667</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.195278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>363291</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.101701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>363947</td>\n",
       "      <td>3.588235</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>0.166274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>363952</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.055023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>363967</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.144064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>363976</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.263597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3243 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id  avg_click_ranking  ranking_weight  ranking_exp_weight\n",
       "0           2022           4.000000        0.200000            0.135335\n",
       "1           2075           5.266667        0.159574            0.071839\n",
       "2           2136           5.333333        0.157895            0.069483\n",
       "3           2231           2.750000        0.266667            0.252840\n",
       "4           2303           3.266667        0.234375            0.195278\n",
       "...          ...                ...             ...                 ...\n",
       "3238      363291           4.571429        0.179487            0.101701\n",
       "3239      363947           3.588235        0.217949            0.166274\n",
       "3240      363952           5.800000        0.147059            0.055023\n",
       "3241      363967           3.875000        0.205128            0.144064\n",
       "3242      363976           2.666667        0.272727            0.263597\n",
       "\n",
       "[3243 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_click_pos = compute_avg_click_pos(train_df)\n",
    "avg_click_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9512cf0c",
   "metadata": {},
   "source": [
    "## Baseline model : popularity-based recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d326261",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_articles_df = (\n",
    "    traintest_df.drop_duplicates(subset=[\"click_article_id\"])\n",
    "    .sort_values(\"article_popularity\", ascending=False)\n",
    "    .filter([\"click_article_id\", \"article_popularity\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b27adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_popularity_dict = dict(\n",
    "    zip(traintest_df[\"click_article_id\"], traintest_df[\"article_popularity\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "67caf6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for each user with the top 5 articles that he hasn't read in the train set relative to the article popularity\n",
    "def get_popularity_recommendations(train_data, top_articles_df, user_id, top_n=5):\n",
    "    # Get the articles read by the user\n",
    "    try:\n",
    "        read_articles = train_data.at[user_id]\n",
    "    except KeyError:\n",
    "        print(f\"User ID {user_id} not found in the training data.\")\n",
    "\n",
    "    # Get the article popularity\n",
    "    articles_not_read = top_articles_df[\n",
    "        ~top_articles_df[\"click_article_id\"].isin(read_articles)\n",
    "    ]\n",
    "    # Filter out the articles already read by the user\n",
    "    recommendations = articles_not_read[\"click_article_id\"].head(top_n)\n",
    "    # Return the top N recommendations\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bd45a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare now the top 5 recommendations for each user in the train set\n",
    "def compare_recommendations(train_data, test_data, top_articles_df, top_n=5):\n",
    "    results = []\n",
    "    for user_id in train_data.index:\n",
    "        train_recommendations = get_popularity_recommendations(\n",
    "            train_data, top_articles_df, user_id=user_id, top_n=top_n\n",
    "        )\n",
    "        try:\n",
    "            # Get the articles read by the user in the test set\n",
    "            test_articles = test_data.at[user_id][:top_n]\n",
    "        except KeyError:\n",
    "            print(f\"User ID {user_id} not found in the test data.\")\n",
    "\n",
    "        # If a article of the top 5 recommandations is in the first 5 articles read by the user in the test set, count it as a hit\n",
    "        # Count the hits\n",
    "        hits = len(set(train_recommendations).intersection(set(test_articles)))\n",
    "        results.append((user_id, hits))\n",
    "    # Create a DataFrame with the results\n",
    "    results = [(user_id, hits) for user_id, hits in results]\n",
    "    if not results:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"hits\"])\n",
    "    # Return a DataFrame with the user_id and the number of hits\n",
    "    return pd.DataFrame(results, columns=[\"user_id\", \"hits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c70f5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(train_data, test_data, top_articles_df, top_n=5):\n",
    "    results = []\n",
    "    for user_id in train_data.index:\n",
    "        recs = get_popularity_recommendations(\n",
    "            train_data, top_articles_df, user_id, top_n\n",
    "        )\n",
    "        try:\n",
    "            test_articles = set(test_data.at[user_id][:top_n])\n",
    "        except KeyError:\n",
    "            test_articles = set()\n",
    "        recs_set = set(recs)\n",
    "        hits = len(recs_set.intersection(test_articles))\n",
    "        precision = hits / top_n if top_n > 0 else 0\n",
    "        recall = hits / len(test_articles) if test_articles else 0\n",
    "        results.append((user_id, precision, recall))\n",
    "    return pd.DataFrame(results, columns=[\"user_id\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4620a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = compare_recommendations(train, test, top_articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "db14d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute precision and recall at k for the recommendations for the test set\n",
    "precision_recall_scores = precision_recall_at_k(train, test, top_articles_df, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bc83e91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.10144188327611575), np.float64(0.10482262547000162))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_scores[\"precision\"].mean(), precision_recall_scores[\"recall\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6eb17",
   "metadata": {},
   "source": [
    "# Baseline model : co-occurrence-based recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005231ea",
   "metadata": {},
   "source": [
    "Parfait ! Voici une version **complète en Python** d’un système de recommandation **non personnalisé**, basé sur les **co-occurrences d’articles**, **pondérées par la récence de lecture** (par rapport à une date de split).\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Objectif\n",
    "\n",
    "À partir d’un historique utilisateur (articles + dates de lecture), recommander les **5 articles les plus pertinents**, en pondérant la contribution de chaque article lu selon sa **récence**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Données simulées\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ⚙️ Co-occurrences simulées\n",
    "cooccurrences = pd.DataFrame({\n",
    "    'article_1': ['a', 'a', 'b', 'b', 'c', 'd', 'e'],\n",
    "    'article_2': ['x', 'y', 'x', 'z', 'y', 'w', 'z'],\n",
    "    'count':     [5,   2,   3,   4,   1,   6,   2]\n",
    "})\n",
    "\n",
    "# 📚 Historique de lecture de l'utilisateur (avant la date de split)\n",
    "history = pd.DataFrame({\n",
    "    'article': ['a', 'b', 'c', 'd', 'e'],\n",
    "    'read_date': pd.to_datetime([\n",
    "        '2023-12-01', '2023-12-15', '2023-12-25', '2023-12-30', '2023-12-31'\n",
    "    ])\n",
    "})\n",
    "\n",
    "split_date = pd.to_datetime('2024-01-01')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Étapes de recommandation\n",
    "\n",
    "### 1. Calcul des pondérations de récence\n",
    "\n",
    "```python\n",
    "# Plus l'article a été lu récemment, plus il est important\n",
    "history['days_before_split'] = (split_date - history['read_date']).dt.days\n",
    "history['recency_weight'] = np.exp(-0.1 * history['days_before_split'])  # base e⁻⁰.¹ᵈ\n",
    "```\n",
    "\n",
    "### 2. Extraire les co-occurrences liées aux articles lus\n",
    "\n",
    "```python\n",
    "# Garder les paires contenant un article lu\n",
    "mask = cooccurrences['article_1'].isin(history['article']) | cooccurrences['article_2'].isin(history['article'])\n",
    "related = cooccurrences[mask].copy()\n",
    "\n",
    "# Identifier l'article lu (source) et la suggestion (cible)\n",
    "def get_source_target(row):\n",
    "    if row['article_1'] in history['article'].values:\n",
    "        return row['article_1'], row['article_2']\n",
    "    else:\n",
    "        return row['article_2'], row['article_1']\n",
    "\n",
    "related[['source', 'suggestion']] = related.apply(get_source_target, axis=1, result_type='expand')\n",
    "```\n",
    "\n",
    "### 3. Ajouter la pondération de récence à chaque ligne\n",
    "\n",
    "```python\n",
    "# Fusionner avec les pondérations des articles lus\n",
    "related = related.merge(history[['article', 'recency_weight']], how='left', left_on='source', right_on='article')\n",
    "\n",
    "# Score final = cooccurrence * pondération de récence\n",
    "related['score'] = related['count'] * related['recency_weight']\n",
    "```\n",
    "\n",
    "### 4. Agrégation des scores et top 5\n",
    "\n",
    "```python\n",
    "# Aggrégation par suggestion (hors articles déjà lus)\n",
    "final_scores = (\n",
    "    related[~related['suggestion'].isin(history['article'])]\n",
    "    .groupby('suggestion')['score']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Top 5 recommandations\n",
    "top5 = final_scores.head(5)\n",
    "\n",
    "print(\"📌 Recommandations pondérées par récence :\")\n",
    "print(top5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Exemple de sortie possible\n",
    "\n",
    "```text\n",
    "📌 Recommandations pondérées par récence :\n",
    "x    1.3\n",
    "z    0.9\n",
    "w    0.5\n",
    "y    0.3\n",
    "Name: score, dtype: float64\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Ce que tu peux améliorer ensuite\n",
    "\n",
    "* Ajouter un **filtrage par contenu** ou thématique.\n",
    "* Penser à une normalisation des co-occurrences (TF-IDF-like).\n",
    "* Ajouter un **paramètre de température** ou un facteur de **diversité**.\n",
    "* Passer à **PySpark** si tu as des millions de lignes.\n",
    "\n",
    "---\n",
    "\n",
    "Souhaite-tu que je t'aide à encapsuler ce pipeline dans une classe ou fonction réutilisable ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482855e",
   "metadata": {},
   "source": [
    "Calcul des co-occurences par fréquence d'apparition des articles dans les sessions\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3aa3e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       article_1  article_2  count\n",
      "0          64329     272143    677\n",
      "1         199198     272143    649\n",
      "2         160974     162655    517\n",
      "3         198659     272143    505\n",
      "4         160974     300470    484\n",
      "...          ...        ...    ...\n",
      "73644     107227     285524      1\n",
      "73645     107227     285719      1\n",
      "73646     107227     286321      1\n",
      "73647     107233     156620      1\n",
      "73648     107218     235990      1\n",
      "\n",
      "[73649 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# Isolate each session with a list of articles clicked\n",
    "sessions = train_df_10.groupby(\"session_id\")[\"click_article_id\"].agg(list)\n",
    "\n",
    "# Étape 1 : générer toutes les paires d’articles par session (ordre non important)\n",
    "pairs = []\n",
    "for session in sessions:\n",
    "    for a, b in combinations(session, 2):\n",
    "        # Étape 2 : trier chaque paire pour ignorer l’ordre\n",
    "        pair = tuple(sorted((a, b)))\n",
    "        pairs.append(pair)\n",
    "\n",
    "# Étape 3 : compter les paires avec pandas\n",
    "df = pd.DataFrame(pairs, columns=[\"article_1\", \"article_2\"])\n",
    "pair_counts = df.value_counts().reset_index(name=\"count\")\n",
    "\n",
    "print(pair_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2086f611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "user_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "click_article_id",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "5fc1301e-e227-4b22-b8d3-00749a267423",
       "rows": [
        [
         "6",
         "[202436, 288431, 160474, 59704, 162300, 166283, 285343, 64329, 70594, 70033, 235230, 336476, 206168, 206934, 70646, 156560, 157078, 233717, 313431, 233717, 168623, 123909, 203288, 283392, 202478, 236444, 236951]"
        ],
        [
         "17",
         "[157861, 314770, 324823, 300473, 161907, 168623, 124350, 124734, 124667, 300048, 162017, 283238, 214474, 129434, 158640, 274204, 118548, 330970, 31520]"
        ],
        [
         "25",
         "[128289, 129960, 235230, 271551, 298687, 59057, 288320]"
        ],
        [
         "26",
         "[119592, 168868, 272660, 160974, 160974, 156619, 285342, 284547, 285648, 286161, 140971, 31836, 233595, 59057]"
        ],
        [
         "42",
         "[145166, 157861, 75825, 107216, 107216, 313996, 285331, 336380, 129434]"
        ],
        [
         "47",
         "[96663, 158064, 59673, 182513, 156963, 363967, 158641, 65399, 124671, 225416]"
        ],
        [
         "50",
         "[96663, 284847, 235132, 284844, 158082, 16129, 160974, 162655, 182394, 64329, 284901, 272143, 235230, 158192, 206462, 284442, 157078, 58193, 233717, 42883, 71076, 159762, 293301, 85396, 157844, 96755, 293114, 283238, 30064, 160129, 286350, 87192, 87231, 87192, 233658]"
        ],
        [
         "55",
         "[13634, 157078, 14030, 14869, 65579, 14888, 14392, 15306]"
        ],
        [
         "59",
         "[234853, 234995, 284463, 354701, 236613, 354701, 285343, 324823, 118753, 235886, 206462, 220302, 235846, 272143, 199197, 361349, 234318, 108940, 308930, 313996, 108856, 129434, 2764, 205832, 235990, 235872, 31520]"
        ],
        [
         "61",
         "[284847, 288431, 96663, 237524, 285379]"
        ],
        [
         "74",
         "[205859, 119592, 119592, 202436, 338340, 124749, 283238]"
        ],
        [
         "77",
         "[288431, 68866, 87144, 336431, 160474, 284463, 261680, 261680, 336430, 118948, 58628, 272660, 313504, 284844, 50864, 324823, 299697, 168623, 233549, 108855, 124749, 338340, 96480, 124194, 271400, 30730, 338339, 160940, 124549, 235689, 272218, 284985, 87224, 299841, 87228, 129434]"
        ],
        [
         "104",
         "[205897, 119592, 32652, 31666, 30446, 124749, 124748, 160983, 31520]"
        ],
        [
         "117",
         "[237620, 162286, 16129, 57543, 162765, 162655, 282959, 156447, 336607, 284901, 282959, 284088, 237257, 202322, 214710, 283776, 234411, 284422, 293432, 70457, 158114, 284985, 237356, 234392, 235689, 202632, 157623, 63307, 161506, 235525, 234994, 31520, 162718]"
        ],
        [
         "121",
         "[235840, 157541, 288431, 206027, 208048, 236444, 236951, 293050, 313996, 233997, 161411, 83534]"
        ],
        [
         "126",
         "[255575, 256007, 272660, 273464, 313504, 256163, 36161, 284547, 233717, 206934, 168623, 168798, 84136, 257291, 124177, 36161, 119193, 235652]"
        ],
        [
         "128",
         "[96663, 119592, 313504, 235132, 140720, 156624, 336476, 129029, 285342, 338350, 157844, 283928, 293114, 293050, 235689, 158906, 96755, 293114, 198225, 129434, 338339, 83534, 62567, 233658, 129434, 299841]"
        ],
        [
         "144",
         "[284847, 284463, 284463, 288270, 202528, 272660, 96755, 128551, 156355, 272218, 129434, 332385, 31520, 123756, 284020, 133160, 156355, 70397, 59383, 160129, 336223, 160324]"
        ],
        [
         "145",
         "[119592, 202557, 300470, 298310, 64329, 218337, 156560, 83893, 158906, 297885, 313996, 235689, 284985]"
        ],
        [
         "146",
         "[119592, 68866, 220466, 284463, 336430, 272143, 160974, 301030, 83534, 336380, 286128, 202308, 87231, 157825, 161801, 299841, 161801, 270799, 31836, 288320, 336223, 48403]"
        ],
        [
         "149",
         "[145166, 304583, 118948, 284463, 272660, 31677, 313504, 235132, 284844, 220466, 162655, 199198, 216102, 198659, 261476, 166283, 156560, 162655, 336476, 129029, 160132, 161100, 123909, 95972, 299152, 32750, 337435, 156625, 236951, 124177, 124177, 124749, 235652, 214283]"
        ],
        [
         "153",
         "[157541, 288431, 160974, 162300, 272143, 338350, 235230, 286161, 156723, 214476, 123909, 214393, 124177, 236951, 284422, 157844, 123826, 338340, 336314, 70999, 208430, 233997, 293114, 214206, 161506, 236553, 336223, 48403]"
        ],
        [
         "154",
         "[96663, 108854, 160474, 270857, 357587, 273395, 199198, 199198, 182394, 182394, 224658, 338350, 118683, 36208, 73413, 293301, 284421, 36080, 36685, 293114, 313995, 313995, 83534, 108856, 133160, 286128, 354022, 351662, 351416, 158047, 63307, 336223, 260254, 59057, 277133, 285754]"
        ],
        [
         "184",
         "[108854, 285023, 58628, 313504, 160974, 156808, 162655, 160417, 107039, 156447, 236171, 162338, 235230, 83565, 206934, 237334, 235230, 91369, 124228, 69377, 283088, 124176, 124748, 124350, 123289, 207797, 283722, 300715, 235451, 158114, 168703, 250751, 234392, 293114, 214474, 272218]"
        ],
        [
         "188",
         "[288645, 199197, 284474, 337266, 124228, 123909, 273473, 123289, 273473, 123756, 209653]"
        ],
        [
         "191",
         "[119592, 202436, 336431, 59704, 59758, 118180, 235440, 58193, 338350, 199197, 156560, 313431, 113978, 220302, 207982, 207994, 206407, 157078, 161602, 206462, 123614, 123289, 124350]"
        ],
        [
         "200",
         "[288431, 108854, 157077, 331738, 123757, 124748, 240233, 214474, 211856, 31520, 336380, 233658, 159177]"
        ],
        [
         "203",
         "[96663, 284847, 274388, 288431, 168701, 160417, 156808, 64329, 166581, 272143, 272660, 235230, 271551, 187065, 175040, 16132, 337435, 214800, 285379, 284088, 357895, 64352, 124748, 124177, 172576, 124194, 124350, 124749, 96755, 286209, 286180, 283392, 158906, 214311, 336380, 83534, 351587]"
        ],
        [
         "214",
         "[108854, 119592, 96663, 233917, 121359, 336380, 129434, 62567, 31520]"
        ],
        [
         "215",
         "[284847, 119592, 68866, 160974, 124176, 124350, 338340, 87232, 63307]"
        ],
        [
         "248",
         "[119592, 304583, 157541, 156355, 97082, 304664, 9216, 313995, 30730, 272218, 156355, 303565, 202308]"
        ],
        [
         "253",
         "[159359, 96663, 277681, 207540, 107182, 168623, 214476, 124749, 293114, 272218, 284985, 160940, 68924, 156355]"
        ],
        [
         "264",
         "[235840, 251735, 161178, 160974, 87234, 235132, 220466, 160974]"
        ],
        [
         "290",
         "[108854, 236682, 233769, 233717, 300356, 124748, 119192, 357940, 283392, 71542, 336380, 128879, 285446, 5254]"
        ],
        [
         "293",
         "[284847, 288431, 68866, 156543, 59758, 209723, 235230, 199197]"
        ],
        [
         "295",
         "[106819, 237620, 284666, 285023, 338350, 157332, 83565, 71500, 272143, 284421, 199197, 187065, 57779, 158646, 233717, 66390, 124749, 254740, 336380, 157825]"
        ],
        [
         "300",
         "[202436, 205897, 160474, 284846, 236613, 270857, 162655, 272143, 226569, 161584, 160417, 71500, 272143, 107179, 61809, 208145, 235430, 214709, 283392, 208430, 124176, 214109, 208430, 284985, 57740, 70564, 272218]"
        ],
        [
         "311",
         "[48973, 5353, 254489, 160417, 124337, 124749, 123818, 123818, 5387, 47718, 288320, 31604, 30032, 187014, 5366, 299463, 119193, 361611, 289186]"
        ],
        [
         "324",
         "[96663, 108856, 301030, 124749, 156355, 107060]"
        ],
        [
         "332",
         "[108854, 202436, 160974, 300470, 202557, 199198, 272143, 198659, 284901, 284622, 64329, 299697, 30730, 337735, 114853, 195007, 299146, 299423, 293050, 239762, 156355, 160940, 233658, 160129, 59057, 288320]"
        ],
        [
         "333",
         "[108854, 119592, 215861, 124228, 331738, 282964, 168623, 276970, 203890, 124350, 124667, 297885, 162017, 108856, 284985, 199766, 129434, 156355, 338339, 83534, 158642, 336380, 87224, 233658, 305094, 48403, 270229, 59057]"
        ],
        [
         "338",
         "[68866, 363291, 87165, 87178, 175040, 272143, 198659, 162655, 166581, 284901, 284312, 224658, 224864, 225198, 218144, 217852, 324823, 299697, 64329, 10034, 285648, 2907, 30730, 173807, 338339, 307448]"
        ],
        [
         "345",
         "[284847, 96663, 145166, 313504, 87203, 64329, 285343, 175040, 166581, 199198, 198659, 160131, 293455, 168623, 124177, 198144, 124749, 283238, 233658, 299841, 87211]"
        ],
        [
         "359",
         "[66457, 205897, 65373, 63657, 70594, 63657, 236220, 111210, 202289, 61486, 208235, 159177, 299841, 73445]"
        ],
        [
         "367",
         "[48973, 235840, 62445, 160974, 160974, 49012, 47999, 47696, 162655, 254489, 272660, 48852, 272143, 198659, 48520, 48392, 36400, 48605, 124228]"
        ],
        [
         "371",
         "[159359, 157541, 237620, 58193, 285435, 107179, 168623, 233717, 156355, 97433, 30446]"
        ],
        [
         "383",
         "[145166, 129960, 158461, 199198, 272143, 224658, 166581, 336476, 187065, 32750, 156723, 108855, 323550, 214800, 283392, 124194, 129434, 332385, 272218, 313995, 315088, 324355, 301030, 362258, 348113, 198199]"
        ],
        [
         "384",
         "[288431, 145166, 87231, 157825, 129434, 336380, 87236, 87205]"
        ],
        [
         "448",
         "[202436, 284847, 272660, 285023, 64329, 166581, 313431, 199197, 124549, 124352, 123757, 123290, 123818]"
        ],
        [
         "468",
         "[166552, 87178, 161178, 160974, 87178, 313504, 233717, 201773, 208082, 36394, 140659, 119099, 286209, 140803, 214137, 338340, 160940, 313996, 20025, 21293, 303563, 288320]"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10195
       }
      },
      "text/plain": [
       "user_id\n",
       "6         [202436, 288431, 160474, 59704, 162300, 166283...\n",
       "17        [157861, 314770, 324823, 300473, 161907, 16862...\n",
       "25        [128289, 129960, 235230, 271551, 298687, 59057...\n",
       "26        [119592, 168868, 272660, 160974, 160974, 15661...\n",
       "42        [145166, 157861, 75825, 107216, 107216, 313996...\n",
       "                                ...                        \n",
       "253111                                     [118391, 336223]\n",
       "253370                                       [271261, 7863]\n",
       "253511                                             [225019]\n",
       "253567                                     [336223, 270229]\n",
       "253663                                             [336223]\n",
       "Name: click_article_id, Length: 10195, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_10.groupby(\"user_id\")[\"click_article_id\"].agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771a73e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        article_1  article_2  count\n",
      "0          160974     162655   1600\n",
      "1          160974     272143   1498\n",
      "2          160974     160974   1478\n",
      "3          123909     160974   1469\n",
      "4          156560     160974   1368\n",
      "...           ...        ...    ...\n",
      "487903     362950     363026      1\n",
      "487904     362950     363234      1\n",
      "487905     363026     363234      1\n",
      "487906     363026     363952      1\n",
      "487907       2022      49204      1\n",
      "\n",
      "[487908 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Isolate each session with a list of articles clicked\n",
    "sessions = train_df_10.groupby(\"user_id\")[\"click_article_id\"].agg(list)\n",
    "\n",
    "# Étape 1 : générer toutes les paires d’articles par session (ordre non important)\n",
    "pairs = []\n",
    "for session in sessions:\n",
    "    for a, b in combinations(session, 2):\n",
    "        # Étape 2 : trier chaque paire pour ignorer l’ordre\n",
    "        pair = tuple(sorted((a, b)))\n",
    "        pairs.append(pair)\n",
    "\n",
    "# Étape 3 : compter les paires avec pandas\n",
    "df = pd.DataFrame(pairs, columns=[\"article_1\", \"article_2\"])\n",
    "pair_counts = df.value_counts().reset_index(name=\"count\")\n",
    "\n",
    "print(pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09eccbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cooccurrence_recommendations(user_id, train_df, pair_counts, top_n=5):\n",
    "    # Get the user's history of articles read\n",
    "    history = train_df[train_df[\"user_id\"] == user_id][\"click_article_id\"].unique()\n",
    "    if len(history) == 0:\n",
    "        print(f\"No history found for user_id {user_id}\")\n",
    "        return []\n",
    "\n",
    "    # Select pairs containing an article from the user's history\n",
    "    mask = pair_counts[\"article_1\"].isin(history) | pair_counts[\"article_2\"].isin(\n",
    "        history\n",
    "    )\n",
    "    related_pairs = pair_counts[mask].copy()\n",
    "\n",
    "    # Identify the candidate article in each pair\n",
    "    def get_other(row):\n",
    "        if row[\"article_1\"] in history:\n",
    "            return row[\"article_2\"]\n",
    "        else:\n",
    "            return row[\"article_1\"]\n",
    "\n",
    "    related_pairs[\"candidate\"] = related_pairs.apply(get_other, axis=1)\n",
    "\n",
    "    # Aggregate scores (sum of co-occurrences)\n",
    "    recommendations = (\n",
    "        related_pairs.groupby(\"candidate\")[\"count\"].sum().sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    # Remove articles already read\n",
    "    recommendations = recommendations[~recommendations.index.isin(history)]\n",
    "\n",
    "    # Return the top N recommendations as a list\n",
    "    return recommendations.head(top_n).index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87dcc47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute the recall@5 and precision@5 for the co-occurrence-based recommendations\n",
    "def evaluate_cooccurrence_recommendations(train_df, test_df, pair_counts, top_n=5):\n",
    "    results = []\n",
    "    for user_id in train_df[\"user_id\"].unique():\n",
    "        recs = get_cooccurrence_recommendations(user_id, train_df, pair_counts, top_n)\n",
    "        try:\n",
    "            test_articles = set(\n",
    "                test_df[test_df[\"user_id\"] == user_id][\"click_article_id\"]\n",
    "            )\n",
    "        except KeyError:\n",
    "            test_articles = set()\n",
    "        recs_set = set(recs)\n",
    "        hits = len(recs_set.intersection(test_articles))\n",
    "        precision = hits / top_n if top_n > 0 else 0\n",
    "        recall = hits / len(test_articles) if test_articles else 0\n",
    "        results.append((user_id, precision, recall))\n",
    "    return pd.DataFrame(results, columns=[\"user_id\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dada52eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.002491417361451692), np.float64(0.000967933102562286))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the precision and recall for the co-occurrence-based recommendations\n",
    "cooccurrence_scores = evaluate_cooccurrence_recommendations(\n",
    "    train_df_10, test_df_10, pair_counts, top_n=5\n",
    ")\n",
    "cooccurrence_scores[\"precision\"].mean(), cooccurrence_scores[\"recall\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d94c58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.0008435507601765573), np.float64(0.00030655404848596346))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the precision and recall for the co-occurrence-based recommendations\n",
    "cooccurrence_scores = evaluate_cooccurrence_recommendations(\n",
    "    train_df_10, test_df_10, pair_counts, top_n=5\n",
    ")\n",
    "cooccurrence_scores[\"precision\"].mean(), cooccurrence_scores[\"recall\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2b45a",
   "metadata": {},
   "source": [
    "# Content based recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ac4925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings_df\n",
    "embeddings_arr = pd.read_pickle(WORK_DIR / \"articles_embeddings.pickle\")\n",
    "embeddings_df = pd.DataFrame(embeddings_arr)\n",
    "valid_articles_ids = (\n",
    "    traintest_df[\"click_article_id\"].drop_duplicates().sort_values(ascending=True)\n",
    ")\n",
    "embeddings_filtered = embeddings_df.loc[\n",
    "    embeddings_df.index.intersection(valid_articles_ids), :\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e1f7b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pca on embeddings_filtered\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings_filtered)\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "embeddings_pca = pca.fit_transform(embeddings_scaled)\n",
    "embeddings_pca_df = pd.DataFrame(embeddings_pca, index=embeddings_filtered.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ccf7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def recommend_content_based(\n",
    "    train_df, embeddings_filtered, SPLIT_DATE, alpha=0.1, beta=0.1, top_n=5\n",
    "):\n",
    "    recommendations = defaultdict(list)\n",
    "    user_ids = train_df[\"user_id\"].unique()\n",
    "\n",
    "    def recency_weight(date_str, ref_date, alpha):\n",
    "        delta = (ref_date - date_str).days\n",
    "        return np.exp(-alpha * delta)\n",
    "\n",
    "    def ranking_weight(position, beta):\n",
    "        return np.exp(-beta * (position - 1))\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        user_df = train_df.loc[train_df[\"user_id\"] == user_id]\n",
    "        weighted_sum = np.zeros(embeddings_filtered.shape[1])\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for row in user_df.itertuples(index=False):\n",
    "            article_id = row.click_article_id\n",
    "            click_date = row.click_timestamp\n",
    "            position = row.click_ranking\n",
    "\n",
    "            w_recency = recency_weight(click_date, SPLIT_DATE, alpha)\n",
    "            w_position = ranking_weight(position, beta)\n",
    "\n",
    "            weighted_sum += (\n",
    "                w_recency * w_position * embeddings_filtered.loc[article_id, :].values\n",
    "            )\n",
    "            total_weight += w_recency * w_position\n",
    "\n",
    "        if total_weight == 0:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "\n",
    "        weighted_sum /= total_weight\n",
    "        similarities = cosine_similarity(\n",
    "            embeddings_filtered.values, weighted_sum.reshape(1, -1)\n",
    "        ).flatten()\n",
    "        sim_df = pd.DataFrame(\n",
    "            {\"article_id\": embeddings_filtered.index, \"similarity\": similarities}\n",
    "        )\n",
    "        read_articles = set(user_df[\"click_article_id\"])\n",
    "        sim_df = sim_df[~sim_df[\"article_id\"].isin(read_articles)]\n",
    "        top_recommendations = sim_df.nlargest(top_n, \"similarity\")\n",
    "        recommendations[user_id] = list(top_recommendations[\"article_id\"])\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "862ec8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommend_content_based(\n",
    "    train_df,\n",
    "    embeddings_filtered,\n",
    "    datetime(2017, 10, 10, 0, 0, 0),\n",
    "    alpha=0.1,\n",
    "    beta=0.1,\n",
    "    top_n=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f4c9025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_pca = recommend_content_based(\n",
    "    train_df,\n",
    "    embeddings_pca_df,\n",
    "    datetime(2017, 10, 10, 0, 0, 0),\n",
    "    alpha=0.1,\n",
    "    beta=0.1,\n",
    "    top_n=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3e994a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_beta = recommend_content_based(\n",
    "    train_df,\n",
    "    embeddings_filtered,\n",
    "    datetime(2017, 10, 10, 0, 0, 0),\n",
    "    alpha=0.2,\n",
    "    beta=0.5,\n",
    "    top_n=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b3f32569",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_beta_pca = recommend_content_based(\n",
    "    train_df,\n",
    "    embeddings_pca_df,\n",
    "    datetime(2017, 10, 10, 0, 0, 0),\n",
    "    alpha=0.2,\n",
    "    beta=0.5,\n",
    "    top_n=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcd2b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_user_recommendations(user_id, recommendations, test_df, top_n=5):\n",
    "    \"\"\"\n",
    "    Compare recommendations to test set for a user and compute precision, recall, and f1@top_n.\n",
    "    \"\"\"\n",
    "    # Get articles read by user in test set\n",
    "    test_articles = test_df.loc[\n",
    "        test_df[\"user_id\"] == user_id, \"click_article_id\"\n",
    "    ].values\n",
    "    recs = recommendations.get(user_id, [])[:top_n]\n",
    "    if len(test_articles) == 0 or len(recs) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    # Take only top_n recommendations and test articles\n",
    "    test_set = set(test_articles)\n",
    "\n",
    "    hits = set(recs) & test_set\n",
    "    precision = len(hits) / len(recs)\n",
    "    recall = len(hits) / len(test_set)\n",
    "    f1 = (\n",
    "        (2 * precision * recall / (precision + recall))\n",
    "        if (precision + recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77fba6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average precision, recall, and f1 for all users\n",
    "def evaluate_all_users(recommendations, test_df, top_n=5):\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1s = []\n",
    "\n",
    "    for user_id in test_df[\"user_id\"].unique():\n",
    "        precision, recall, f1 = evaluate_user_recommendations(\n",
    "            user_id, recommendations, test_df, top_n\n",
    "        )\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1s.append(f1)\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_f1 = np.mean(all_f1s)\n",
    "\n",
    "    return avg_precision, avg_recall, avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "32ab98e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@5: 0.0142, Average Recall@5: 0.0058, Average F1@5: 0.0077\n"
     ]
    }
   ],
   "source": [
    "# Compute precision, recall, and f1 for all users\n",
    "avg_precision, avg_recall, avg_f1 = evaluate_all_users(\n",
    "    recommendations, test_df, top_n=5\n",
    ")\n",
    "print(\n",
    "    f\"Average Precision@5: {avg_precision:.4f}, Average Recall@5: {avg_recall:.4f}, Average F1@5: {avg_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6f7e766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@5: 0.0147, Average Recall@5: 0.0061, Average F1@5: 0.0081\n"
     ]
    }
   ],
   "source": [
    "# Compute precision, recall, and f1 for all users\n",
    "avg_precision, avg_recall, avg_f1 = evaluate_all_users(\n",
    "    recommendations_pca, test_df, top_n=5\n",
    ")\n",
    "print(\n",
    "    f\"Average Precision@5: {avg_precision:.4f}, Average Recall@5: {avg_recall:.4f}, Average F1@5: {avg_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b9c741f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@5: 0.0149, Average Recall@5: 0.0061, Average F1@5: 0.0082\n"
     ]
    }
   ],
   "source": [
    "# Compute precision, recall, and f1 for all users\n",
    "avg_precision, avg_recall, avg_f1 = evaluate_all_users(\n",
    "    recommendations_beta, test_df, top_n=5\n",
    ")\n",
    "print(\n",
    "    f\"Average Precision@5: {avg_precision:.4f}, Average Recall@5: {avg_recall:.4f}, Average F1@5: {avg_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5954313d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@5: 0.0147, Average Recall@5: 0.0061, Average F1@5: 0.0082\n"
     ]
    }
   ],
   "source": [
    "# Compute precision, recall, and f1 for all users\n",
    "avg_precision, avg_recall, avg_f1 = evaluate_all_users(\n",
    "    recommendations_beta_pca, test_df, top_n=5\n",
    ")\n",
    "print(\n",
    "    f\"Average Precision@5: {avg_precision:.4f}, Average Recall@5: {avg_recall:.4f}, Average F1@5: {avg_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e709a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_content_based_avg(train_df, embeddings_filtered, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend articles based on the average embedding of articles read by each user.\n",
    "\n",
    "    Args:\n",
    "        train_df: DataFrame with columns ['user_id', 'click_article_id']\n",
    "        embeddings_filtered: DataFrame, index=article_id, values=embedding vectors\n",
    "        top_n: int, number of recommendations\n",
    "\n",
    "    Returns:\n",
    "        dict[user_id, list of article_id]: top N recommendations per user\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    recommendations = {}\n",
    "    user_ids = train_df[\"user_id\"].unique()\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        user_articles = train_df.loc[train_df[\"user_id\"] == user_id, \"click_article_id\"]\n",
    "        read_articles = set(user_articles)\n",
    "        if not read_articles:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "\n",
    "        # Get embeddings for articles read by the user\n",
    "        user_embs = embeddings_filtered.loc[\n",
    "            embeddings_filtered.index.intersection(read_articles)\n",
    "        ].values\n",
    "\n",
    "        if user_embs.shape[0] == 0:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "\n",
    "        # Compute average embedding\n",
    "        avg_emb = user_embs.mean(axis=0).reshape(1, -1)\n",
    "\n",
    "        # Compute cosine similarity with all candidate articles\n",
    "        similarities = cosine_similarity(embeddings_filtered.values, avg_emb).flatten()\n",
    "        sim_df = pd.DataFrame(\n",
    "            {\"article_id\": embeddings_filtered.index, \"similarity\": similarities}\n",
    "        )\n",
    "\n",
    "        # Exclude already read articles\n",
    "        sim_df = sim_df[~sim_df[\"article_id\"].isin(read_articles)]\n",
    "        top_recommendations = sim_df.nlargest(top_n, \"similarity\")\n",
    "        recommendations[user_id] = list(top_recommendations[\"article_id\"])\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c75245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute recommendations using the average embedding method\n",
    "recommendations_avg = recommend_content_based_avg(\n",
    "    train_df, embeddings_filtered, top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3337cf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@5: 0.0055, Average Recall@5: 0.0057, Average F1@5: 0.0056\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the average embedding recommendations\n",
    "avg_precision, avg_recall, avg_f1 = evaluate_all_users(\n",
    "    recommendations_avg, test_df, top_n=5\n",
    ")\n",
    "print(\n",
    "    f\"Average Precision@5: {avg_precision:.4f}, Average Recall@5: {avg_recall:.4f}, Average F1@5: {avg_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5fa8fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def recommend_content_based(\n",
    "    train_df,\n",
    "    embeddings_filtered,\n",
    "    SPLIT_DATE,\n",
    "    alpha=0.2,\n",
    "    beta=0.5,\n",
    "    top_n=5,\n",
    "    article_popularity=None,\n",
    "    lambda_=0.7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Content-based recommender with hybrid popularity.\n",
    "    Args:\n",
    "        article_popularity: dict or pd.Series {article_id: popularity_score}\n",
    "        lambda_: float, weight for similarity (1-lambda_ for popularity)\n",
    "    \"\"\"\n",
    "    recommendations = defaultdict(list)\n",
    "    user_ids = train_df[\"user_id\"].unique()\n",
    "\n",
    "    def recency_weight(date_str, ref_date, alpha):\n",
    "        delta = (ref_date - date_str).days\n",
    "        return np.exp(-alpha * delta)\n",
    "\n",
    "    def position_weight(position, beta):\n",
    "        return np.exp(-beta * (position - 1))\n",
    "\n",
    "    # Normalise la popularité si fournie\n",
    "    if article_popularity is not None:\n",
    "        if isinstance(article_popularity, dict):\n",
    "            pop_series = pd.Series(article_popularity)\n",
    "        else:\n",
    "            pop_series = article_popularity\n",
    "        pop_min = pop_series.min()\n",
    "        pop_max = pop_series.max()\n",
    "        pop_norm = (pop_series - pop_min) / (pop_max - pop_min + 1e-9)\n",
    "    else:\n",
    "        pop_norm = None\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        user_df = train_df.loc[train_df[\"user_id\"] == user_id]\n",
    "        weighted_sum = np.zeros(embeddings_filtered.shape[1])\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for row in user_df.itertuples(index=False):\n",
    "            article_id = row.click_article_id\n",
    "            click_date = row.click_timestamp\n",
    "            position = row.click_ranking\n",
    "\n",
    "            w_recency = recency_weight(click_date, SPLIT_DATE, alpha)\n",
    "            w_position = position_weight(position, beta)\n",
    "\n",
    "            weighted_sum += (\n",
    "                w_recency * w_position * embeddings_filtered.loc[article_id, :].values\n",
    "            )\n",
    "            total_weight += w_recency * w_position\n",
    "\n",
    "        if total_weight == 0:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "\n",
    "        weighted_sum /= total_weight\n",
    "        similarities = cosine_similarity(\n",
    "            embeddings_filtered.values, weighted_sum.reshape(1, -1)\n",
    "        ).flatten()\n",
    "        sim_df = pd.DataFrame(\n",
    "            {\"article_id\": embeddings_filtered.index, \"similarity\": similarities}\n",
    "        )\n",
    "        read_articles = set(user_df[\"click_article_id\"])\n",
    "        sim_df = sim_df[~sim_df[\"article_id\"].isin(read_articles)]\n",
    "\n",
    "        # Ajout de la popularité normalisée\n",
    "        if pop_norm is not None:\n",
    "            sim_df = sim_df.join(pop_norm.rename(\"popularity_norm\"), on=\"article_id\")\n",
    "            sim_df[\"popularity_norm\"] = sim_df[\"popularity_norm\"].fillna(0)\n",
    "            sim_df[\"hybrid_score\"] = (\n",
    "                lambda_ * sim_df[\"similarity\"]\n",
    "                + (1 - lambda_) * sim_df[\"popularity_norm\"]\n",
    "            )\n",
    "            top_recommendations = sim_df.nlargest(top_n, \"hybrid_score\")\n",
    "        else:\n",
    "            top_recommendations = sim_df.nlargest(top_n, \"similarity\")\n",
    "\n",
    "        recommendations[user_id] = list(top_recommendations[\"article_id\"])\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b2e23983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_popularity doit être un dict ou une Series {article_id: score}\n",
    "recommendations_with_pop = recommend_content_based(\n",
    "    train_df,\n",
    "    embeddings_filtered,\n",
    "    SPLIT_DATE,\n",
    "    alpha=0.2,\n",
    "    beta=0.5,\n",
    "    top_n=5,\n",
    "    article_popularity=article_popularity_dict,\n",
    "    lambda_=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8734db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "73c4db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def recommend_content_based_position(\n",
    "    train_df, embeddings_filtered, SPLIT_DATE, alpha=0.1, beta=0.1, top_n=5\n",
    "):\n",
    "    recommendations = defaultdict(list)\n",
    "    user_ids = train_df[\"user_id\"].unique()\n",
    "\n",
    "    def recency_weight(date_str, ref_date, alpha):\n",
    "        delta = (ref_date - date_str).days\n",
    "        return np.exp(-alpha * delta)\n",
    "\n",
    "    def position_weight(position, beta):\n",
    "        return np.exp(-beta * (position - 1))\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        user_df = train_df.loc[train_df[\"user_id\"] == user_id]\n",
    "        len_position = len(user_df)\n",
    "        weighted_sum = np.zeros(embeddings_filtered.shape[1])\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for i, row in enumerate(user_df.itertuples(index=False)):\n",
    "            article_id = row.click_article_id\n",
    "            click_date = row.click_timestamp\n",
    "            position = len_position - i  # Invert position for ranking\n",
    "\n",
    "            w_recency = recency_weight(click_date, SPLIT_DATE, alpha)\n",
    "            w_position = position_weight(position, beta)\n",
    "\n",
    "            weighted_sum += (\n",
    "                w_recency * w_position * embeddings_filtered.loc[article_id, :].values\n",
    "            )\n",
    "            total_weight += w_recency * w_position\n",
    "\n",
    "        if total_weight == 0:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "\n",
    "        weighted_sum /= total_weight\n",
    "        similarities = cosine_similarity(\n",
    "            embeddings_filtered.values, weighted_sum.reshape(1, -1)\n",
    "        ).flatten()\n",
    "        sim_df = pd.DataFrame(\n",
    "            {\"article_id\": embeddings_filtered.index, \"similarity\": similarities}\n",
    "        )\n",
    "        read_articles = set(user_df[\"click_article_id\"])\n",
    "        sim_df = sim_df[~sim_df[\"article_id\"].isin(read_articles)]\n",
    "        top_recommendations = sim_df.nlargest(top_n, \"similarity\")\n",
    "        recommendations[user_id] = list(top_recommendations[\"article_id\"])\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1612a31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@5: 0.0136, Average Recall@5: 0.0055, Average F1@5: 0.0075\n"
     ]
    }
   ],
   "source": [
    "# compute recommendations using the average embedding method\n",
    "recommendations_with_position = recommend_content_based_position(\n",
    "    train_df, embeddings_filtered, SPLIT_DATE, alpha=0.2, beta=0.5, top_n=5\n",
    ")\n",
    "\n",
    "# Evaluate the average embedding recommendations\n",
    "avg_precision, avg_recall, avg_f1 = evaluate_all_users(\n",
    "    recommendations_with_position, test_df, top_n=5\n",
    ")\n",
    "print(\n",
    "    f\"Average Precision@5: {avg_precision:.4f}, Average Recall@5: {avg_recall:.4f}, Average F1@5: {avg_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ca81af9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "user_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "session_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "session_start",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "session_size",
         "rawType": "int8",
         "type": "integer"
        },
        {
         "name": "click_article_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "click_timestamp",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "click_environment",
         "rawType": "int8",
         "type": "integer"
        },
        {
         "name": "click_deviceGroup",
         "rawType": "int8",
         "type": "integer"
        },
        {
         "name": "click_os",
         "rawType": "int8",
         "type": "integer"
        },
        {
         "name": "click_country",
         "rawType": "int8",
         "type": "integer"
        },
        {
         "name": "click_region",
         "rawType": "int8",
         "type": "integer"
        },
        {
         "name": "click_referrer_type",
         "rawType": "int8",
         "type": "integer"
        },
        {
         "name": "click_timestamp_M",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "click_timestamp_H",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "click_timestamp_D",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "article_popularity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "article_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "read_duration",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "click_ranking",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "first_click_timestamp",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "article_recence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "popularity_weighted_by_recence_sqrt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "popularity_weighted_by_recence",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "article_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "category_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "created_at_ts",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "publisher_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "words_count",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "ref": "a5db6578-b2fd-4aa5-be9f-80d479a40b61",
       "rows": [
        [
         "0",
         "59",
         "1506826329267796",
         "2017-10-01 02:52:09",
         "2",
         "234853",
         "2017-10-01 03:00:00.026000",
         "4",
         "3",
         "2",
         "1",
         "21",
         "1",
         "0",
         "3",
         "1",
         "2.1752363728970904e-05",
         "65",
         "194.437",
         "1",
         "2017-10-01 03:00:00.026000",
         "0",
         "0.0",
         "0.0",
         "234853",
         "375",
         "2017-09-30 12:18:09",
         "0",
         "140"
        ],
        [
         "1",
         "154",
         "1506826793323891",
         "2017-10-01 02:59:53",
         "2",
         "96663",
         "2017-10-01 03:00:04.207000",
         "4",
         "3",
         "2",
         "1",
         "25",
         "7",
         "0",
         "3",
         "1",
         "0.0014948893658048158",
         "4467",
         "184.551",
         "1",
         "2017-10-01 03:00:04.207000",
         "0",
         "0.0",
         "0.0",
         "96663",
         "209",
         "2017-09-30 16:13:45",
         "0",
         "206"
        ],
        [
         "2",
         "59",
         "1506826329267796",
         "2017-10-01 02:52:09",
         "2",
         "234995",
         "2017-10-01 03:00:30.026000",
         "4",
         "3",
         "2",
         "1",
         "21",
         "1",
         "0",
         "3",
         "1",
         "1.4724676985764919e-05",
         "44",
         "178.736",
         "2",
         "2017-10-01 03:00:30.026000",
         "0",
         "0.0",
         "0.0",
         "234995",
         "375",
         "2017-09-30 12:07:16",
         "0",
         "155"
        ],
        [
         "3",
         "149",
         "1506826780136886",
         "2017-10-01 02:59:40",
         "2",
         "145166",
         "2017-10-01 03:00:31.267000",
         "4",
         "3",
         "20",
         "1",
         "26",
         "2",
         "0",
         "3",
         "1",
         "0.00026504418574376854",
         "792",
         "207.6985",
         "1",
         "2017-10-01 03:00:31.267000",
         "0",
         "0.0",
         "0.0",
         "145166",
         "269",
         "2017-09-30 15:27:52",
         "0",
         "180"
        ],
        [
         "4",
         "154",
         "1506826793323891",
         "2017-10-01 02:59:53",
         "2",
         "108854",
         "2017-10-01 03:00:34.207000",
         "4",
         "3",
         "2",
         "1",
         "25",
         "7",
         "0",
         "3",
         "1",
         "0.0005628842429558317",
         "1682",
         "140.818",
         "2",
         "2017-10-01 03:00:34.207000",
         "0",
         "0.0",
         "0.0",
         "108854",
         "230",
         "2017-09-30 21:09:43",
         "0",
         "167"
        ],
        [
         "5",
         "153",
         "1506826788896890",
         "2017-10-01 02:59:48",
         "3",
         "157541",
         "2017-10-01 03:00:47.697000",
         "4",
         "3",
         "20",
         "1",
         "24",
         "2",
         "0",
         "3",
         "1",
         "0.00012315184388094296",
         "368",
         "105.326",
         "1",
         "2017-10-01 03:00:28.020000",
         "0",
         "0.0",
         "0.0",
         "157541",
         "281",
         "2017-09-30 19:41:58",
         "0",
         "280"
        ],
        [
         "6",
         "104",
         "1506826610425841",
         "2017-10-01 02:56:50",
         "2",
         "205897",
         "2017-10-01 03:00:50.126000",
         "4",
         "3",
         "2",
         "1",
         "5",
         "1",
         "0",
         "3",
         "1",
         "9.738365915585434e-05",
         "291",
         "290.117",
         "1",
         "2017-10-01 03:00:31.702000",
         "0",
         "0.0",
         "0.0",
         "205897",
         "331",
         "2017-09-30 13:33:01",
         "0",
         "278"
        ],
        [
         "7",
         "121",
         "1506826653149858",
         "2017-10-01 02:57:33",
         "3",
         "235840",
         "2017-10-01 03:00:59.494000",
         "2",
         "3",
         "20",
         "10",
         "28",
         "2",
         "0",
         "3",
         "1",
         "0.00029282028096691597",
         "875",
         "343.808",
         "1",
         "2017-10-01 03:00:59.494000",
         "0",
         "0.0",
         "0.0",
         "235840",
         "375",
         "2017-09-30 21:43:59",
         "0",
         "159"
        ],
        [
         "8",
         "149",
         "1506826780136886",
         "2017-10-01 02:59:40",
         "2",
         "304583",
         "2017-10-01 03:01:01.267000",
         "4",
         "3",
         "20",
         "1",
         "26",
         "2",
         "1",
         "3",
         "1",
         "1.5059328735441394e-05",
         "45",
         "296.3315",
         "2",
         "2017-10-01 03:01:01.267000",
         "0",
         "0.0",
         "0.0",
         "304583",
         "429",
         "2017-09-30 10:15:07",
         "0",
         "148"
        ],
        [
         "9",
         "6",
         "1506825553218743",
         "2017-10-01 02:39:13",
         "2",
         "202436",
         "2017-10-01 03:01:12.215000",
         "4",
         "3",
         "20",
         "1",
         "21",
         "2",
         "1",
         "3",
         "1",
         "5.689079744500082e-05",
         "170",
         "273.585",
         "1",
         "2017-10-01 03:00:14.140000",
         "0",
         "0.0",
         "0.0",
         "202436",
         "327",
         "2017-09-30 16:15:43",
         "0",
         "333"
        ],
        [
         "10",
         "104",
         "1506826610425841",
         "2017-10-01 02:56:50",
         "2",
         "119592",
         "2017-10-01 03:01:20.126000",
         "4",
         "3",
         "2",
         "1",
         "5",
         "1",
         "1",
         "3",
         "1",
         "0.001503590311296404",
         "4493",
         "160.637",
         "2",
         "2017-10-01 03:00:18.863000",
         "0",
         "0.0",
         "0.0",
         "119592",
         "247",
         "2017-09-30 15:11:56",
         "0",
         "239"
        ],
        [
         "11",
         "146",
         "1506826762418883",
         "2017-10-01 02:59:22",
         "2",
         "119592",
         "2017-10-01 03:01:25.647000",
         "4",
         "1",
         "17",
         "1",
         "24",
         "2",
         "1",
         "3",
         "1",
         "0.001503590311296404",
         "4493",
         "160.637",
         "1",
         "2017-10-01 03:00:18.863000",
         "0",
         "0.0",
         "0.0",
         "119592",
         "247",
         "2017-09-30 15:11:56",
         "0",
         "239"
        ],
        [
         "12",
         "6",
         "1506825553218743",
         "2017-10-01 02:39:13",
         "2",
         "288431",
         "2017-10-01 03:01:42.215000",
         "4",
         "3",
         "20",
         "1",
         "21",
         "2",
         "1",
         "3",
         "1",
         "3.8150299463118195e-05",
         "114",
         "200.805",
         "2",
         "2017-10-01 03:00:33.968000",
         "0",
         "0.0",
         "0.0",
         "288431",
         "418",
         "2017-09-30 08:15:57",
         "0",
         "208"
        ],
        [
         "13",
         "47",
         "1506826146134784",
         "2017-10-01 02:49:06",
         "2",
         "96663",
         "2017-10-01 03:01:44.701000",
         "4",
         "3",
         "20",
         "1",
         "25",
         "1",
         "1",
         "3",
         "1",
         "0.0014948893658048158",
         "4467",
         "184.551",
         "2",
         "2017-10-01 03:00:04.207000",
         "0",
         "0.0",
         "0.0",
         "96663",
         "209",
         "2017-09-30 16:13:45",
         "0",
         "206"
        ],
        [
         "14",
         "25",
         "1506825826220762",
         "2017-10-01 02:43:46",
         "2",
         "128289",
         "2017-10-01 03:01:47.840000",
         "4",
         "3",
         "2",
         "1",
         "13",
         "1",
         "1",
         "3",
         "1",
         "1.171281123867664e-05",
         "35",
         "156.6155",
         "1",
         "2017-10-01 03:01:47.840000",
         "0",
         "0.0",
         "0.0",
         "128289",
         "252",
         "2017-09-30 19:45:17",
         "0",
         "165"
        ],
        [
         "15",
         "146",
         "1506826762418883",
         "2017-10-01 02:59:22",
         "2",
         "68866",
         "2017-10-01 03:01:55.647000",
         "4",
         "1",
         "17",
         "1",
         "24",
         "2",
         "1",
         "3",
         "1",
         "0.0005019776245147131",
         "1500",
         "166.626",
         "2",
         "2017-10-01 03:00:58.020000",
         "0",
         "0.0",
         "0.0",
         "68866",
         "136",
         "2017-10-01 00:08:02",
         "0",
         "226"
        ],
        [
         "16",
         "128",
         "1506826683183865",
         "2017-10-01 02:58:03",
         "2",
         "96663",
         "2017-10-01 03:02:08.633000",
         "4",
         "3",
         "20",
         "1",
         "25",
         "7",
         "2",
         "3",
         "1",
         "0.0014948893658048158",
         "4467",
         "184.551",
         "1",
         "2017-10-01 03:00:04.207000",
         "0",
         "0.0",
         "0.0",
         "96663",
         "209",
         "2017-09-30 16:13:45",
         "0",
         "206"
        ],
        [
         "17",
         "25",
         "1506825826220762",
         "2017-10-01 02:43:46",
         "2",
         "129960",
         "2017-10-01 03:02:17.840000",
         "4",
         "3",
         "2",
         "1",
         "13",
         "1",
         "2",
         "3",
         "1",
         "1.9075149731559097e-05",
         "57",
         "227.654",
         "2",
         "2017-10-01 03:02:17.840000",
         "0",
         "0.0",
         "0.0",
         "129960",
         "252",
         "2017-09-30 10:57:07",
         "0",
         "199"
        ],
        [
         "18",
         "184",
         "1506826928133921",
         "2017-10-01 03:02:08",
         "2",
         "108854",
         "2017-10-01 03:02:18.849000",
         "4",
         "3",
         "2",
         "11",
         "28",
         "1",
         "2",
         "3",
         "1",
         "0.0005628842429558317",
         "1682",
         "140.818",
         "1",
         "2017-10-01 03:00:34.207000",
         "0",
         "0.0",
         "0.0",
         "108854",
         "230",
         "2017-09-30 21:09:43",
         "0",
         "167"
        ],
        [
         "19",
         "128",
         "1506826683183865",
         "2017-10-01 02:58:03",
         "2",
         "119592",
         "2017-10-01 03:02:38.633000",
         "4",
         "3",
         "20",
         "1",
         "25",
         "7",
         "2",
         "3",
         "1",
         "0.001503590311296404",
         "4493",
         "160.637",
         "2",
         "2017-10-01 03:00:18.863000",
         "0",
         "0.0",
         "0.0",
         "119592",
         "247",
         "2017-09-30 15:11:56",
         "0",
         "239"
        ],
        [
         "20",
         "153",
         "1506826788896890",
         "2017-10-01 02:59:48",
         "3",
         "288431",
         "2017-10-01 03:03:03.023000",
         "4",
         "3",
         "20",
         "1",
         "24",
         "1",
         "3",
         "3",
         "1",
         "3.8150299463118195e-05",
         "114",
         "200.805",
         "3",
         "2017-10-01 03:00:33.968000",
         "0",
         "0.0",
         "0.0",
         "288431",
         "418",
         "2017-09-30 08:15:57",
         "0",
         "208"
        ],
        [
         "21",
         "191",
         "1506826951395928",
         "2017-10-01 03:02:31",
         "3",
         "119592",
         "2017-10-01 03:03:13.054000",
         "4",
         "1",
         "17",
         "1",
         "12",
         "2",
         "3",
         "3",
         "1",
         "0.001503590311296404",
         "4493",
         "12.227",
         "1",
         "2017-10-01 03:00:18.863000",
         "0",
         "0.0",
         "0.0",
         "119592",
         "247",
         "2017-09-30 15:11:56",
         "0",
         "239"
        ],
        [
         "22",
         "191",
         "1506826951395928",
         "2017-10-01 03:02:31",
         "3",
         "202436",
         "2017-10-01 03:03:25.281000",
         "4",
         "1",
         "17",
         "1",
         "12",
         "2",
         "3",
         "3",
         "1",
         "5.689079744500082e-05",
         "170",
         "273.585",
         "2",
         "2017-10-01 03:00:14.140000",
         "0",
         "0.0",
         "0.0",
         "202436",
         "327",
         "2017-09-30 16:15:43",
         "0",
         "333"
        ],
        [
         "23",
         "42",
         "1506826126337779",
         "2017-10-01 02:48:46",
         "3",
         "145166",
         "2017-10-01 03:03:40.569000",
         "4",
         "3",
         "20",
         "1",
         "25",
         "1",
         "3",
         "3",
         "1",
         "0.00026504418574376854",
         "792",
         "282.837",
         "1",
         "2017-10-01 03:00:31.267000",
         "0",
         "0.0",
         "0.0",
         "145166",
         "269",
         "2017-09-30 15:27:52",
         "0",
         "180"
        ],
        [
         "24",
         "203",
         "1506826998530940",
         "2017-10-01 03:03:18",
         "6",
         "96663",
         "2017-10-01 03:03:40.718000",
         "4",
         "3",
         "2",
         "1",
         "25",
         "2",
         "3",
         "3",
         "1",
         "0.0014948893658048158",
         "4467",
         "184.551",
         "1",
         "2017-10-01 03:00:04.207000",
         "0",
         "0.0",
         "0.0",
         "96663",
         "209",
         "2017-09-30 16:13:45",
         "0",
         "206"
        ],
        [
         "25",
         "214",
         "1506827039326951",
         "2017-10-01 03:03:59",
         "3",
         "108854",
         "2017-10-01 03:04:43.499000",
         "2",
         "3",
         "20",
         "1",
         "13",
         "2",
         "4",
         "3",
         "1",
         "0.0005628842429558317",
         "1682",
         "11.381",
         "1",
         "2017-10-01 03:00:34.207000",
         "0",
         "0.0",
         "0.0",
         "108854",
         "230",
         "2017-09-30 21:09:43",
         "0",
         "167"
        ],
        [
         "26",
         "26",
         "1506825860299763",
         "2017-10-01 02:44:20",
         "3",
         "119592",
         "2017-10-01 03:04:44.599000",
         "4",
         "3",
         "2",
         "1",
         "13",
         "2",
         "4",
         "3",
         "1",
         "0.001503590311296404",
         "4493",
         "128.337",
         "1",
         "2017-10-01 03:00:18.863000",
         "0",
         "0.0",
         "0.0",
         "119592",
         "247",
         "2017-09-30 15:11:56",
         "0",
         "239"
        ],
        [
         "27",
         "214",
         "1506827039326951",
         "2017-10-01 03:03:59",
         "3",
         "119592",
         "2017-10-01 03:04:54.880000",
         "2",
         "3",
         "20",
         "1",
         "13",
         "2",
         "4",
         "3",
         "1",
         "0.001503590311296404",
         "4493",
         "160.637",
         "2",
         "2017-10-01 03:00:18.863000",
         "0",
         "0.0",
         "0.0",
         "119592",
         "247",
         "2017-09-30 15:11:56",
         "0",
         "239"
        ],
        [
         "28",
         "215",
         "1506827039231952",
         "2017-10-01 03:03:59",
         "3",
         "284847",
         "2017-10-01 03:04:57.422000",
         "4",
         "1",
         "17",
         "1",
         "13",
         "2",
         "4",
         "3",
         "1",
         "0.0004909341167753894",
         "1467",
         "224.2405",
         "1",
         "2017-10-01 03:00:23.065000",
         "0",
         "0.0",
         "0.0",
         "284847",
         "412",
         "2017-09-30 20:07:53",
         "0",
         "266"
        ],
        [
         "29",
         "203",
         "1506826998530940",
         "2017-10-01 03:03:18",
         "6",
         "284847",
         "2017-10-01 03:05:22.066000",
         "4",
         "3",
         "2",
         "1",
         "25",
         "1",
         "5",
         "3",
         "1",
         "0.0004909341167753894",
         "1467",
         "60.181",
         "2",
         "2017-10-01 03:00:23.065000",
         "0",
         "0.0",
         "0.0",
         "284847",
         "412",
         "2017-09-30 20:07:53",
         "0",
         "266"
        ],
        [
         "30",
         "214",
         "1506827039326951",
         "2017-10-01 03:03:59",
         "3",
         "96663",
         "2017-10-01 03:05:24.880000",
         "2",
         "3",
         "20",
         "1",
         "13",
         "2",
         "5",
         "3",
         "1",
         "0.0014948893658048158",
         "4467",
         "2177.734",
         "3",
         "2017-10-01 03:00:04.207000",
         "0",
         "0.0",
         "0.0",
         "96663",
         "209",
         "2017-09-30 16:13:45",
         "0",
         "206"
        ],
        [
         "31",
         "144",
         "1506826754203881",
         "2017-10-01 02:59:14",
         "2",
         "284847",
         "2017-10-01 03:06:15.364000",
         "4",
         "1",
         "17",
         "1",
         "9",
         "1",
         "6",
         "3",
         "1",
         "0.0004909341167753894",
         "1467",
         "224.2405",
         "2",
         "2017-10-01 03:00:23.065000",
         "0",
         "0.0",
         "0.0",
         "284847",
         "412",
         "2017-09-30 20:07:53",
         "0",
         "266"
        ],
        [
         "32",
         "203",
         "1506826998530940",
         "2017-10-01 03:03:18",
         "6",
         "274388",
         "2017-10-01 03:06:22.247000",
         "4",
         "3",
         "2",
         "1",
         "25",
         "1",
         "6",
         "3",
         "1",
         "1.9075149731559097e-05",
         "57",
         "156.669",
         "3",
         "2017-10-01 03:06:22.247000",
         "0",
         "0.0",
         "0.0",
         "274388",
         "402",
         "2017-09-30 14:49:07",
         "0",
         "286"
        ],
        [
         "33",
         "121",
         "1506826653149858",
         "2017-10-01 02:57:33",
         "3",
         "157541",
         "2017-10-01 03:06:43.302000",
         "2",
         "3",
         "20",
         "10",
         "28",
         "2",
         "6",
         "3",
         "1",
         "0.00012315184388094296",
         "368",
         "165.4205",
         "2",
         "2017-10-01 03:00:28.020000",
         "0",
         "0.0",
         "0.0",
         "157541",
         "281",
         "2017-09-30 19:41:58",
         "0",
         "280"
        ],
        [
         "34",
         "121",
         "1506826653149858",
         "2017-10-01 02:57:33",
         "3",
         "288431",
         "2017-10-01 03:07:13.302000",
         "2",
         "3",
         "20",
         "10",
         "28",
         "2",
         "7",
         "3",
         "1",
         "3.8150299463118195e-05",
         "114",
         "200.805",
         "3",
         "2017-10-01 03:00:33.968000",
         "0",
         "0.0",
         "0.0",
         "288431",
         "418",
         "2017-09-30 08:15:57",
         "0",
         "208"
        ],
        [
         "35",
         "26",
         "1506825860299763",
         "2017-10-01 02:44:20",
         "3",
         "168868",
         "2017-10-01 03:07:22.936000",
         "4",
         "3",
         "2",
         "1",
         "13",
         "1",
         "7",
         "3",
         "1",
         "0.0001211439333828841",
         "362",
         "163.3435",
         "3",
         "2017-10-01 03:01:24.885000",
         "0",
         "0.0",
         "0.0",
         "168868",
         "297",
         "2017-09-30 18:40:51",
         "0",
         "147"
        ],
        [
         "36",
         "17",
         "1506825696288754",
         "2017-10-01 02:41:36",
         "3",
         "157861",
         "2017-10-01 03:08:46.697000",
         "4",
         "1",
         "17",
         "1",
         "25",
         "1",
         "8",
         "3",
         "1",
         "4.149681695988295e-05",
         "124",
         "238.627",
         "2",
         "2017-10-01 03:08:46.697000",
         "0",
         "0.0",
         "0.0",
         "157861",
         "281",
         "2017-09-30 14:14:42",
         "0",
         "296"
        ],
        [
         "37",
         "42",
         "1506826126337779",
         "2017-10-01 02:48:46",
         "3",
         "157861",
         "2017-10-01 03:08:53.406000",
         "4",
         "3",
         "20",
         "1",
         "25",
         "1",
         "8",
         "3",
         "1",
         "4.149681695988295e-05",
         "124",
         "238.627",
         "3",
         "2017-10-01 03:08:46.697000",
         "0",
         "0.0",
         "0.0",
         "157861",
         "281",
         "2017-09-30 14:14:42",
         "0",
         "296"
        ],
        [
         "38",
         "290",
         "1506827383295027",
         "2017-10-01 03:09:43",
         "3",
         "108854",
         "2017-10-01 03:09:43.168000",
         "4",
         "4",
         "20",
         "1",
         "24",
         "5",
         "9",
         "3",
         "1",
         "0.0005628842429558317",
         "1682",
         "875.872",
         "1",
         "2017-10-01 03:00:34.207000",
         "0",
         "0.0",
         "0.0",
         "108854",
         "230",
         "2017-09-30 21:09:43",
         "0",
         "167"
        ],
        [
         "39",
         "293",
         "1506827389171030",
         "2017-10-01 03:09:49",
         "3",
         "284847",
         "2017-10-01 03:10:22.059000",
         "4",
         "3",
         "20",
         "1",
         "25",
         "2",
         "10",
         "3",
         "1",
         "0.0004909341167753894",
         "1467",
         "251.229",
         "1",
         "2017-10-01 03:00:23.065000",
         "0",
         "0.0",
         "0.0",
         "284847",
         "412",
         "2017-09-30 20:07:53",
         "0",
         "266"
        ],
        [
         "40",
         "295",
         "1506827399898032",
         "2017-10-01 03:09:59",
         "3",
         "106819",
         "2017-10-01 03:10:29.211000",
         "4",
         "3",
         "20",
         "1",
         "6",
         "1",
         "10",
         "3",
         "1",
         "2.2756318978000328e-05",
         "68",
         "1045.904",
         "1",
         "2017-10-01 03:05:29.071000",
         "0",
         "0.0",
         "0.0",
         "106819",
         "228",
         "2017-09-30 21:01:44",
         "0",
         "112"
        ],
        [
         "41",
         "61",
         "1506826358370798",
         "2017-10-01 02:52:38",
         "2",
         "284847",
         "2017-10-01 03:11:33.624000",
         "4",
         "1",
         "17",
         "1",
         "25",
         "2",
         "11",
         "3",
         "1",
         "0.0004909341167753894",
         "1467",
         "224.2405",
         "1",
         "2017-10-01 03:00:23.065000",
         "0",
         "0.0",
         "0.0",
         "284847",
         "412",
         "2017-09-30 20:07:53",
         "0",
         "266"
        ],
        [
         "42",
         "324",
         "1506827503201061",
         "2017-10-01 03:11:43",
         "3",
         "96663",
         "2017-10-01 03:11:58.458000",
         "4",
         "3",
         "2",
         "1",
         "25",
         "2",
         "11",
         "3",
         "1",
         "0.0014948893658048158",
         "4467",
         "184.551",
         "1",
         "2017-10-01 03:00:04.207000",
         "0",
         "0.0",
         "0.0",
         "96663",
         "209",
         "2017-09-30 16:13:45",
         "0",
         "206"
        ],
        [
         "43",
         "61",
         "1506826358370798",
         "2017-10-01 02:52:38",
         "2",
         "288431",
         "2017-10-01 03:12:03.624000",
         "4",
         "1",
         "17",
         "1",
         "25",
         "2",
         "12",
         "3",
         "1",
         "3.8150299463118195e-05",
         "114",
         "200.805",
         "2",
         "2017-10-01 03:00:33.968000",
         "0",
         "0.0",
         "0.0",
         "288431",
         "418",
         "2017-09-30 08:15:57",
         "0",
         "208"
        ],
        [
         "44",
         "300",
         "1506827422653037",
         "2017-10-01 03:10:22",
         "2",
         "202436",
         "2017-10-01 03:12:10.381000",
         "4",
         "3",
         "2",
         "1",
         "13",
         "1",
         "12",
         "3",
         "1",
         "5.689079744500082e-05",
         "170",
         "273.585",
         "1",
         "2017-10-01 03:00:14.140000",
         "0",
         "0.0",
         "0.0",
         "202436",
         "327",
         "2017-09-30 16:15:43",
         "0",
         "333"
        ],
        [
         "45",
         "50",
         "1506826204226787",
         "2017-10-01 02:50:04",
         "2",
         "96663",
         "2017-10-01 03:12:16.127000",
         "4",
         "1",
         "17",
         "1",
         "25",
         "2",
         "12",
         "3",
         "1",
         "0.0014948893658048158",
         "4467",
         "184.551",
         "1",
         "2017-10-01 03:00:04.207000",
         "0",
         "0.0",
         "0.0",
         "96663",
         "209",
         "2017-09-30 16:13:45",
         "0",
         "206"
        ],
        [
         "46",
         "300",
         "1506827422653037",
         "2017-10-01 03:10:22",
         "2",
         "205897",
         "2017-10-01 03:12:40.381000",
         "4",
         "3",
         "2",
         "1",
         "13",
         "1",
         "12",
         "3",
         "1",
         "9.738365915585434e-05",
         "291",
         "290.117",
         "2",
         "2017-10-01 03:00:31.702000",
         "0",
         "0.0",
         "0.0",
         "205897",
         "331",
         "2017-09-30 13:33:01",
         "0",
         "278"
        ],
        [
         "47",
         "50",
         "1506826204226787",
         "2017-10-01 02:50:04",
         "2",
         "284847",
         "2017-10-01 03:12:46.127000",
         "4",
         "1",
         "17",
         "1",
         "25",
         "2",
         "12",
         "3",
         "1",
         "0.0004909341167753894",
         "1467",
         "224.2405",
         "2",
         "2017-10-01 03:00:23.065000",
         "0",
         "0.0",
         "0.0",
         "284847",
         "412",
         "2017-09-30 20:07:53",
         "0",
         "266"
        ],
        [
         "48",
         "332",
         "1506827522550069",
         "2017-10-01 03:12:02",
         "2",
         "108854",
         "2017-10-01 03:13:05.177000",
         "4",
         "1",
         "17",
         "1",
         "24",
         "2",
         "13",
         "3",
         "1",
         "0.0005628842429558317",
         "1682",
         "140.818",
         "1",
         "2017-10-01 03:00:34.207000",
         "0",
         "0.0",
         "0.0",
         "108854",
         "230",
         "2017-09-30 21:09:43",
         "0",
         "167"
        ],
        [
         "49",
         "203",
         "1506826998530940",
         "2017-10-01 03:03:18",
         "6",
         "288431",
         "2017-10-01 03:13:33.680000",
         "4",
         "3",
         "2",
         "1",
         "25",
         "2",
         "13",
         "3",
         "1",
         "3.8150299463118195e-05",
         "114",
         "251.506",
         "4",
         "2017-10-01 03:00:33.968000",
         "0",
         "0.0",
         "0.0",
         "288431",
         "418",
         "2017-09-30 08:15:57",
         "0",
         "208"
        ]
       ],
       "shape": {
        "columns": 28,
        "rows": 181880
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_start</th>\n",
       "      <th>session_size</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>...</th>\n",
       "      <th>click_ranking</th>\n",
       "      <th>first_click_timestamp</th>\n",
       "      <th>article_recence</th>\n",
       "      <th>popularity_weighted_by_recence_sqrt</th>\n",
       "      <th>popularity_weighted_by_recence</th>\n",
       "      <th>article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>created_at_ts</th>\n",
       "      <th>publisher_id</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>1506826329267796</td>\n",
       "      <td>2017-10-01 02:52:09</td>\n",
       "      <td>2</td>\n",
       "      <td>234853</td>\n",
       "      <td>2017-10-01 03:00:00.026</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-01 03:00:00.026</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234853</td>\n",
       "      <td>375</td>\n",
       "      <td>2017-09-30 12:18:09</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>1506826793323891</td>\n",
       "      <td>2017-10-01 02:59:53</td>\n",
       "      <td>2</td>\n",
       "      <td>96663</td>\n",
       "      <td>2017-10-01 03:00:04.207</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-01 03:00:04.207</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96663</td>\n",
       "      <td>209</td>\n",
       "      <td>2017-09-30 16:13:45</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "      <td>1506826329267796</td>\n",
       "      <td>2017-10-01 02:52:09</td>\n",
       "      <td>2</td>\n",
       "      <td>234995</td>\n",
       "      <td>2017-10-01 03:00:30.026</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10-01 03:00:30.026</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234995</td>\n",
       "      <td>375</td>\n",
       "      <td>2017-09-30 12:07:16</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>1506826780136886</td>\n",
       "      <td>2017-10-01 02:59:40</td>\n",
       "      <td>2</td>\n",
       "      <td>145166</td>\n",
       "      <td>2017-10-01 03:00:31.267</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-01 03:00:31.267</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>145166</td>\n",
       "      <td>269</td>\n",
       "      <td>2017-09-30 15:27:52</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154</td>\n",
       "      <td>1506826793323891</td>\n",
       "      <td>2017-10-01 02:59:53</td>\n",
       "      <td>2</td>\n",
       "      <td>108854</td>\n",
       "      <td>2017-10-01 03:00:34.207</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10-01 03:00:34.207</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108854</td>\n",
       "      <td>230</td>\n",
       "      <td>2017-09-30 21:09:43</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181875</th>\n",
       "      <td>11668</td>\n",
       "      <td>1507593499203413</td>\n",
       "      <td>2017-10-09 23:58:19</td>\n",
       "      <td>2</td>\n",
       "      <td>225019</td>\n",
       "      <td>2017-10-09 23:59:35.046</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10-09 00:00:12.195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>225019</td>\n",
       "      <td>354</td>\n",
       "      <td>2017-10-09 16:25:28</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181876</th>\n",
       "      <td>15068</td>\n",
       "      <td>1507592669124580</td>\n",
       "      <td>2017-10-09 23:44:29</td>\n",
       "      <td>4</td>\n",
       "      <td>338129</td>\n",
       "      <td>2017-10-09 23:59:41.302</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-10-09 18:23:42.835</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>338129</td>\n",
       "      <td>437</td>\n",
       "      <td>2017-10-09 15:20:51</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181877</th>\n",
       "      <td>3086</td>\n",
       "      <td>1507593391142309</td>\n",
       "      <td>2017-10-09 23:56:31</td>\n",
       "      <td>6</td>\n",
       "      <td>119193</td>\n",
       "      <td>2017-10-09 23:59:43.075</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10-07 21:56:52.677</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>119193</td>\n",
       "      <td>247</td>\n",
       "      <td>2017-10-09 16:17:03</td>\n",
       "      <td>0</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181878</th>\n",
       "      <td>34646</td>\n",
       "      <td>1507592869316761</td>\n",
       "      <td>2017-10-09 23:47:49</td>\n",
       "      <td>7</td>\n",
       "      <td>161149</td>\n",
       "      <td>2017-10-09 23:59:49.251</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-10-09 17:53:37.335</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>161149</td>\n",
       "      <td>281</td>\n",
       "      <td>2017-10-09 14:48:21</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181879</th>\n",
       "      <td>22068</td>\n",
       "      <td>1507593518264434</td>\n",
       "      <td>2017-10-09 23:58:38</td>\n",
       "      <td>2</td>\n",
       "      <td>59057</td>\n",
       "      <td>2017-10-09 23:59:57.134</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10-09 02:53:56.034</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59057</td>\n",
       "      <td>118</td>\n",
       "      <td>2017-10-09 11:10:45</td>\n",
       "      <td>0</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181880 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id        session_id       session_start  session_size  \\\n",
       "0            59  1506826329267796 2017-10-01 02:52:09             2   \n",
       "1           154  1506826793323891 2017-10-01 02:59:53             2   \n",
       "2            59  1506826329267796 2017-10-01 02:52:09             2   \n",
       "3           149  1506826780136886 2017-10-01 02:59:40             2   \n",
       "4           154  1506826793323891 2017-10-01 02:59:53             2   \n",
       "...         ...               ...                 ...           ...   \n",
       "181875    11668  1507593499203413 2017-10-09 23:58:19             2   \n",
       "181876    15068  1507592669124580 2017-10-09 23:44:29             4   \n",
       "181877     3086  1507593391142309 2017-10-09 23:56:31             6   \n",
       "181878    34646  1507592869316761 2017-10-09 23:47:49             7   \n",
       "181879    22068  1507593518264434 2017-10-09 23:58:38             2   \n",
       "\n",
       "        click_article_id         click_timestamp  click_environment  \\\n",
       "0                 234853 2017-10-01 03:00:00.026                  4   \n",
       "1                  96663 2017-10-01 03:00:04.207                  4   \n",
       "2                 234995 2017-10-01 03:00:30.026                  4   \n",
       "3                 145166 2017-10-01 03:00:31.267                  4   \n",
       "4                 108854 2017-10-01 03:00:34.207                  4   \n",
       "...                  ...                     ...                ...   \n",
       "181875            225019 2017-10-09 23:59:35.046                  4   \n",
       "181876            338129 2017-10-09 23:59:41.302                  4   \n",
       "181877            119193 2017-10-09 23:59:43.075                  2   \n",
       "181878            161149 2017-10-09 23:59:49.251                  4   \n",
       "181879             59057 2017-10-09 23:59:57.134                  4   \n",
       "\n",
       "        click_deviceGroup  click_os  click_country  ...  click_ranking  \\\n",
       "0                       3         2              1  ...              1   \n",
       "1                       3         2              1  ...              1   \n",
       "2                       3         2              1  ...              2   \n",
       "3                       3        20              1  ...              1   \n",
       "4                       3         2              1  ...              2   \n",
       "...                   ...       ...            ...  ...            ...   \n",
       "181875                  3         2              1  ...              2   \n",
       "181876                  3         2              1  ...              4   \n",
       "181877                  3        20              1  ...              2   \n",
       "181878                  3        20              1  ...              4   \n",
       "181879                  1        17              1  ...              2   \n",
       "\n",
       "         first_click_timestamp  article_recence  \\\n",
       "0      2017-10-01 03:00:00.026                0   \n",
       "1      2017-10-01 03:00:04.207                0   \n",
       "2      2017-10-01 03:00:30.026                0   \n",
       "3      2017-10-01 03:00:31.267                0   \n",
       "4      2017-10-01 03:00:34.207                0   \n",
       "...                        ...              ...   \n",
       "181875 2017-10-09 00:00:12.195                0   \n",
       "181876 2017-10-09 18:23:42.835                0   \n",
       "181877 2017-10-07 21:56:52.677                2   \n",
       "181878 2017-10-09 17:53:37.335                0   \n",
       "181879 2017-10-09 02:53:56.034                0   \n",
       "\n",
       "        popularity_weighted_by_recence_sqrt  popularity_weighted_by_recence  \\\n",
       "0                                  0.000000                        0.000000   \n",
       "1                                  0.000000                        0.000000   \n",
       "2                                  0.000000                        0.000000   \n",
       "3                                  0.000000                        0.000000   \n",
       "4                                  0.000000                        0.000000   \n",
       "...                                     ...                             ...   \n",
       "181875                             0.000000                        0.000000   \n",
       "181876                             0.000000                        0.000000   \n",
       "181877                             0.004642                        0.006565   \n",
       "181878                             0.000000                        0.000000   \n",
       "181879                             0.000000                        0.000000   \n",
       "\n",
       "        article_id  category_id       created_at_ts  publisher_id words_count  \n",
       "0           234853          375 2017-09-30 12:18:09             0         140  \n",
       "1            96663          209 2017-09-30 16:13:45             0         206  \n",
       "2           234995          375 2017-09-30 12:07:16             0         155  \n",
       "3           145166          269 2017-09-30 15:27:52             0         180  \n",
       "4           108854          230 2017-09-30 21:09:43             0         167  \n",
       "...            ...          ...                 ...           ...         ...  \n",
       "181875      225019          354 2017-10-09 16:25:28             0         190  \n",
       "181876      338129          437 2017-10-09 15:20:51             0         187  \n",
       "181877      119193          247 2017-10-09 16:17:03             0         296  \n",
       "181878      161149          281 2017-10-09 14:48:21             0         205  \n",
       "181879       59057          118 2017-10-09 11:10:45             0         191  \n",
       "\n",
       "[181880 rows x 28 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "165d210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def recommend_content_based_category(\n",
    "    train_df,\n",
    "    embeddings_filtered,\n",
    "    SPLIT_DATE,\n",
    "    alpha=0.23,\n",
    "    beta=0.5,\n",
    "    top_n=5,\n",
    "    category_weight=2.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build user profile by weighting articles from the user's most frequent category higher.\n",
    "    \"\"\"\n",
    "    recommendations = defaultdict(list)\n",
    "    user_ids = train_df[\"user_id\"].unique()\n",
    "\n",
    "    def recency_weight(date_str, ref_date, alpha):\n",
    "        delta = (ref_date - date_str).days\n",
    "        return np.exp(-alpha * delta)\n",
    "\n",
    "    def ranking_weight(position, beta):\n",
    "        return np.exp(-beta * (position - 1))\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        user_df = train_df.loc[train_df[\"user_id\"] == user_id]\n",
    "        weighted_sum = np.zeros(embeddings_filtered.shape[1])\n",
    "        total_weight = 0.0\n",
    "\n",
    "        # Find user's most frequent category\n",
    "        if \"category_id\" in user_df.columns:\n",
    "            most_freq_cat = user_df[\"category_id\"].mode().iloc[0]\n",
    "        else:\n",
    "            most_freq_cat = None\n",
    "\n",
    "        for row in user_df.itertuples(index=False):\n",
    "            article_id = row.click_article_id\n",
    "            click_date = row.click_timestamp\n",
    "            position = row.click_ranking\n",
    "            category_id = row.category_id if hasattr(row, \"category_id\") else None\n",
    "\n",
    "            w_recency = recency_weight(click_date, SPLIT_DATE, alpha)\n",
    "            w_position = ranking_weight(position, beta)\n",
    "            w_cat = category_weight if (category_id == most_freq_cat) else 1.0\n",
    "\n",
    "            weighted_sum += (\n",
    "                w_recency\n",
    "                * w_position\n",
    "                * w_cat\n",
    "                * embeddings_filtered.loc[article_id, :].values\n",
    "            )\n",
    "            total_weight += w_recency * w_position * w_cat\n",
    "\n",
    "        if total_weight == 0:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "\n",
    "        weighted_sum /= total_weight\n",
    "        similarities = cosine_similarity(\n",
    "            embeddings_filtered.values, weighted_sum.reshape(1, -1)\n",
    "        ).flatten()\n",
    "        sim_df = pd.DataFrame(\n",
    "            {\"article_id\": embeddings_filtered.index, \"similarity\": similarities}\n",
    "        )\n",
    "        read_articles = set(user_df[\"click_article_id\"])\n",
    "        sim_df = sim_df[~sim_df[\"article_id\"].isin(read_articles)]\n",
    "        top_recommendations = sim_df.nlargest(top_n, \"similarity\")\n",
    "        recommendations[user_id] = list(top_recommendations[\"article_id\"])\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "45d0e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@5: 0.0144, Average Recall@5: 0.0058, Average F1@5: 0.0078\n"
     ]
    }
   ],
   "source": [
    "# compute recommendations using the average embedding method\n",
    "recommendations_with_category = recommend_content_based_category(\n",
    "    train_df, embeddings_filtered, SPLIT_DATE, alpha=0.2, beta=0.5, top_n=5\n",
    ")\n",
    "\n",
    "# Evaluate the average embedding recommendations\n",
    "avg_precision, avg_recall, avg_f1 = evaluate_all_users(\n",
    "    recommendations_with_category, test_df, top_n=5\n",
    ")\n",
    "print(\n",
    "    f\"Average Precision@5: {avg_precision:.4f}, Average Recall@5: {avg_recall:.4f}, Average F1@5: {avg_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640c94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da63cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_content_based_last_article(train_df, embeddings_filtered, top_n=5):\n",
    "    \"\"\"\n",
    "    Simple content-based recommender using only the last article read by each user.\n",
    "    No recency weighting, no click ranking weighting - just pure similarity to the last article.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataframe with user interactions\n",
    "        embeddings_filtered: DataFrame with article embeddings (index=article_id)\n",
    "        top_n: Number of recommendations to return\n",
    "        \n",
    "    Returns:\n",
    "        dict: {user_id: [list of recommended article_ids]}\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    recommendations = {}\n",
    "    user_ids = train_df[\"user_id\"].unique()\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        # Get user's interaction history sorted by timestamp (ascending)\n",
    "        user_df = train_df.loc[train_df[\"user_id\"] == user_id].sort_values(\"click_timestamp\")\n",
    "        \n",
    "        if len(user_df) == 0:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "            \n",
    "        # Get the last article read by the user\n",
    "        last_article_id = user_df.iloc[-1][\"click_article_id\"]\n",
    "        \n",
    "        # Get all articles read by the user (to exclude from recommendations)\n",
    "        read_articles = set(user_df[\"click_article_id\"])\n",
    "        \n",
    "        # Check if the last article has embeddings\n",
    "        if last_article_id not in embeddings_filtered.index:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "            \n",
    "        # Get embedding of the last article\n",
    "        last_article_embedding = embeddings_filtered.loc[last_article_id].values.reshape(1, -1)\n",
    "        \n",
    "        # Compute cosine similarity with all other articles\n",
    "        similarities = cosine_similarity(embeddings_filtered.values, last_article_embedding).flatten()\n",
    "        \n",
    "        # Create DataFrame with article IDs and similarities\n",
    "        sim_df = pd.DataFrame({\n",
    "            \"article_id\": embeddings_filtered.index,\n",
    "            \"similarity\": similarities\n",
    "        })\n",
    "        \n",
    "        # Exclude articles already read by the user\n",
    "        sim_df = sim_df[~sim_df[\"article_id\"].isin(read_articles)]\n",
    "        \n",
    "        # Get top N recommendations\n",
    "        top_recommendations = sim_df.nlargest(top_n, \"similarity\")\n",
    "        recommendations[user_id] = list(top_recommendations[\"article_id\"])\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bccb6427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendations based on last article only...\n"
     ]
    }
   ],
   "source": [
    "# Generate recommendations using only the last article consulted\n",
    "print(\"Generating recommendations based on last article only...\")\n",
    "recommendations_last_article = recommend_content_based_last_article(\n",
    "    train_df, embeddings_filtered, top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "451f4936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Last Article Content-Based Recommendations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Article - Average Precision@5: 0.0110\n",
      "Last Article - Average Recall@5: 0.0047\n",
      "Last Article - Average F1@5: 0.0062\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the last article recommendations\n",
    "print(\"Evaluating Last Article Content-Based Recommendations...\")\n",
    "last_article_avg_precision, last_article_avg_recall, last_article_avg_f1 = evaluate_all_users(\n",
    "    recommendations_last_article, test_df, top_n=5\n",
    ")\n",
    "print(f\"Last Article - Average Precision@5: {last_article_avg_precision:.4f}\")\n",
    "print(f\"Last Article - Average Recall@5: {last_article_avg_recall:.4f}\")\n",
    "print(f\"Last Article - Average F1@5: {last_article_avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11612c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison of all content-based approaches\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONTENT-BASED RECOMMENDERS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get metrics for content-based with category weighting\n",
    "content_cat_avg_precision, content_cat_avg_recall, content_cat_avg_f1 = evaluate_all_users(\n",
    "    recommendations_with_category, test_df, top_n=5\n",
    ")\n",
    "\n",
    "# Get metrics for average embedding approach\n",
    "content_avg_avg_precision, content_avg_avg_recall, content_avg_avg_f1 = evaluate_all_users(\n",
    "    recommendations_avg, test_df, top_n=5\n",
    ")\n",
    "\n",
    "# Get metrics for weighted approach (alpha=0.2, beta=0.5)\n",
    "content_weighted_avg_precision, content_weighted_avg_recall, content_weighted_avg_f1 = evaluate_all_users(\n",
    "    recommendations_beta, test_df, top_n=5\n",
    ")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "content_comparison = pd.DataFrame({\n",
    "    'Content-Based Approach': [\n",
    "        'Weighted (α=0.1, β=0.1)',\n",
    "        'Weighted (α=0.2, β=0.5)', \n",
    "        'Average Embeddings',\n",
    "        'Category Weighted',\n",
    "        'Hybrid with Popularity',\n",
    "        'Last Article Only'\n",
    "    ],\n",
    "    'Precision@5': [\n",
    "        # First get the original weighted approach scores\n",
    "        0.0,  # You'll need to compute this from recommendations\n",
    "        content_weighted_avg_precision,\n",
    "        content_avg_avg_precision,\n",
    "        content_cat_avg_precision,\n",
    "        # Get the hybrid with popularity scores\n",
    "        0.0,  # You'll need to compute this from recommendations_with_pop\n",
    "        last_article_avg_precision\n",
    "    ],\n",
    "    'Recall@5': [\n",
    "        0.0,  # You'll need to compute this\n",
    "        content_weighted_avg_recall,\n",
    "        content_avg_avg_recall,\n",
    "        content_cat_avg_recall,\n",
    "        0.0,  # You'll need to compute this\n",
    "        last_article_avg_recall\n",
    "    ],\n",
    "    'F1@5': [\n",
    "        0.0,  # You'll need to compute this\n",
    "        content_weighted_avg_f1,\n",
    "        content_avg_avg_f1,\n",
    "        content_cat_avg_f1,\n",
    "        0.0,  # You'll need to compute this\n",
    "        last_article_avg_f1\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(content_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the missing evaluations for comprehensive comparison\n",
    "print(\"Computing missing metrics for complete comparison...\")\n",
    "\n",
    "# Evaluate original weighted approach (α=0.1, β=0.1)\n",
    "original_weighted_avg_precision, original_weighted_avg_recall, original_weighted_avg_f1 = evaluate_all_users(\n",
    "    recommendations, test_df, top_n=5\n",
    ")\n",
    "\n",
    "# Evaluate hybrid with popularity approach\n",
    "hybrid_pop_avg_precision, hybrid_pop_avg_recall, hybrid_pop_avg_f1 = evaluate_all_users(\n",
    "    recommendations_with_pop, test_df, top_n=5\n",
    ")\n",
    "\n",
    "# Update the comparison with complete data\n",
    "content_comparison_complete = pd.DataFrame({\n",
    "    'Content-Based Approach': [\n",
    "        'Weighted (α=0.1, β=0.1)',\n",
    "        'Weighted (α=0.2, β=0.5)', \n",
    "        'Average Embeddings',\n",
    "        'Category Weighted',\n",
    "        'Hybrid with Popularity',\n",
    "        'Last Article Only'\n",
    "    ],\n",
    "    'Precision@5': [\n",
    "        original_weighted_avg_precision,\n",
    "        content_weighted_avg_precision,\n",
    "        content_avg_avg_precision,\n",
    "        content_cat_avg_precision,\n",
    "        hybrid_pop_avg_precision,\n",
    "        last_article_avg_precision\n",
    "    ],\n",
    "    'Recall@5': [\n",
    "        original_weighted_avg_recall,\n",
    "        content_weighted_avg_recall,\n",
    "        content_avg_avg_recall,\n",
    "        content_cat_avg_recall,\n",
    "        hybrid_pop_avg_recall,\n",
    "        last_article_avg_recall\n",
    "    ],\n",
    "    'F1@5': [\n",
    "        original_weighted_avg_f1,\n",
    "        content_weighted_avg_f1,\n",
    "        content_avg_avg_f1,\n",
    "        content_cat_avg_f1,\n",
    "        hybrid_pop_avg_f1,\n",
    "        last_article_avg_f1\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nCOMPLETE CONTENT-BASED COMPARISON:\")\n",
    "print(content_comparison_complete.to_string(index=False))\n",
    "\n",
    "# Find the best performing approach\n",
    "best_precision_idx = content_comparison_complete['Precision@5'].idxmax()\n",
    "best_recall_idx = content_comparison_complete['Recall@5'].idxmax()\n",
    "best_f1_idx = content_comparison_complete['F1@5'].idxmax()\n",
    "\n",
    "print(f\"\\nBest Precision@5: {content_comparison_complete.iloc[best_precision_idx]['Content-Based Approach']} \"\n",
    "      f\"({content_comparison_complete.iloc[best_precision_idx]['Precision@5']:.4f})\")\n",
    "print(f\"Best Recall@5: {content_comparison_complete.iloc[best_recall_idx]['Content-Based Approach']} \"\n",
    "      f\"({content_comparison_complete.iloc[best_recall_idx]['Recall@5']:.4f})\")\n",
    "print(f\"Best F1@5: {content_comparison_complete.iloc[best_f1_idx]['Content-Based Approach']} \"\n",
    "      f\"({content_comparison_complete.iloc[best_f1_idx]['F1@5']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the last article approach characteristics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LAST ARTICLE APPROACH ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check how many users have only one article in their history\n",
    "single_article_users = train_df.groupby('user_id').size()\n",
    "users_with_single_article = (single_article_users == 1).sum()\n",
    "total_users = len(single_article_users)\n",
    "\n",
    "print(f\"Users with only 1 article in history: {users_with_single_article}/{total_users} ({100*users_with_single_article/total_users:.1f}%)\")\n",
    "print(f\"Users with multiple articles: {total_users - users_with_single_article}/{total_users} ({100*(total_users - users_with_single_article)/total_users:.1f}%)\")\n",
    "\n",
    "# Analyze diversity of the last article recommendations\n",
    "print(\"\\nDiversity Analysis for Last Article Recommendations:\")\n",
    "last_article_analysis = analyze_recommendation_diversity(recommendations_last_article, embeddings_filtered)\n",
    "for key, value in last_article_analysis.items():\n",
    "    print(f\"{key}: {value:.4f}\" if value is not None else f\"{key}: {value}\")\n",
    "\n",
    "# Compare with average embedding approach diversity\n",
    "print(\"\\nComparison with Average Embedding Approach:\")\n",
    "avg_embedding_analysis = analyze_recommendation_diversity(recommendations_avg, embeddings_filtered)\n",
    "print(f\"Last Article Coverage: {last_article_analysis['coverage']:.4f}\")\n",
    "print(f\"Average Embedding Coverage: {avg_embedding_analysis['coverage']:.4f}\")\n",
    "print(f\"Last Article Diversity: {last_article_analysis['avg_diversity']:.4f}\" if last_article_analysis['avg_diversity'] else \"Last Article Diversity: None\")\n",
    "print(f\"Average Embedding Diversity: {avg_embedding_analysis['avg_diversity']:.4f}\" if avg_embedding_analysis['avg_diversity'] else \"Average Embedding Diversity: None\")\n",
    "\n",
    "# Check temporal recency of last articles\n",
    "print(f\"\\nTemporal Analysis:\")\n",
    "last_article_dates = train_df.groupby('user_id')['click_timestamp'].max()\n",
    "days_before_split = (SPLIT_DATE - last_article_dates).dt.days\n",
    "print(f\"Average days between last article and split: {days_before_split.mean():.1f}\")\n",
    "print(f\"Median days between last article and split: {days_before_split.median():.1f}\")\n",
    "print(f\"Users with last article < 7 days before split: {(days_before_split < 7).sum()}/{len(days_before_split)} ({100*(days_before_split < 7).sum()/len(days_before_split):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4d465",
   "metadata": {},
   "source": [
    "## Last Article Content-Based Recommender Analysis\n",
    "\n",
    "### Approach Description:\n",
    "The **Last Article** recommender is the simplest content-based approach that:\n",
    "- Uses only the **most recent article** consulted by each user\n",
    "- No recency weighting or click ranking considerations\n",
    "- Pure cosine similarity between the last article's embedding and all other articles\n",
    "- Excludes articles already read by the user\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ **Simplicity**: No hyperparameters to tune\n",
    "- ✅ **Computational efficiency**: Only one similarity computation per user\n",
    "- ✅ **Interpretability**: Easy to understand and explain\n",
    "- ✅ **Real-time ready**: Fast inference for new recommendations\n",
    "- ✅ **Fresh preferences**: Focuses on user's most current interests\n",
    "\n",
    "**Potential Limitations:**\n",
    "- ❌ **Ignores historical preferences**: Doesn't consider user's broader taste\n",
    "- ❌ **Vulnerable to outliers**: One atypical article can skew all recommendations\n",
    "- ❌ **No temporal context**: Misses evolving user behavior patterns\n",
    "- ❌ **Limited for sparse users**: Poor performance for users with very few interactions\n",
    "\n",
    "### Performance Implications:\n",
    "This approach works best when:\n",
    "- Users have consistent, focused interests\n",
    "- The last article is representative of current preferences  \n",
    "- Content similarity is a strong predictor of user satisfaction\n",
    "- Real-time speed is prioritized over recommendation sophistication\n",
    "\n",
    "The comparison with other content-based approaches will show whether simplicity sometimes outperforms complexity in recommendation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bdd4e1",
   "metadata": {},
   "source": [
    "# Collaborative filtering recommender system\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1848bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_weighted_interaction_matrix(train_df, SPLIT_DATE, alpha=0.1, beta=0.1, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Create a weighted user-item interaction matrix incorporating:\n",
    "    - Click frequency (implicit feedback)\n",
    "    - Recency weighting \n",
    "    - Click ranking weighting\n",
    "    - Overall interaction strength\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataframe\n",
    "        SPLIT_DATE: Reference date for recency calculation\n",
    "        alpha: Recency decay parameter\n",
    "        beta: Click ranking decay parameter  \n",
    "        gamma: Overall scaling factor\n",
    "    \"\"\"\n",
    "    \n",
    "    def recency_weight(date_str, ref_date, alpha):\n",
    "        delta = (ref_date - date_str).days\n",
    "        return np.exp(-alpha * delta)\n",
    "\n",
    "    def ranking_weight(position, beta):\n",
    "        return np.exp(-beta * (position - 1))\n",
    "    \n",
    "    # Calculate weights for each interaction\n",
    "    train_df_weighted = train_df.copy()\n",
    "    train_df_weighted['recency_w'] = train_df_weighted['click_timestamp'].apply(\n",
    "        lambda x: recency_weight(x, SPLIT_DATE, alpha)\n",
    "    )\n",
    "    train_df_weighted['ranking_w'] = train_df_weighted['click_ranking'].apply(\n",
    "        lambda x: ranking_weight(x, beta)\n",
    "    )\n",
    "    \n",
    "    # Combined weight: recency * ranking * base_strength\n",
    "    train_df_weighted['interaction_weight'] = (\n",
    "        gamma * train_df_weighted['recency_w'] * train_df_weighted['ranking_w']\n",
    "    )\n",
    "    \n",
    "    # Group by user-article pairs and sum weights (for multiple clicks on same article)\n",
    "    interaction_weights = (\n",
    "        train_df_weighted.groupby(['user_id', 'click_article_id'])['interaction_weight']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Create categorical codes for sparse matrix\n",
    "    user_codes = interaction_weights['user_id'].astype('category').cat.codes\n",
    "    article_codes = interaction_weights['click_article_id'].astype('category').cat.codes\n",
    "    \n",
    "    # Create mappings\n",
    "    user_categories = interaction_weights['user_id'].astype('category').cat.categories\n",
    "    article_categories = interaction_weights['click_article_id'].astype('category').cat.categories\n",
    "    \n",
    "    user_id_map = dict(enumerate(user_categories))\n",
    "    article_id_map = dict(enumerate(article_categories))\n",
    "    user_id_invmap = {v: k for k, v in user_id_map.items()}\n",
    "    article_id_invmap = {v: k for k, v in article_id_map.items()}\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    weights = interaction_weights['interaction_weight'].values\n",
    "    interaction_matrix = csr_matrix(\n",
    "        (weights, (user_codes, article_codes)),\n",
    "        shape=(len(user_categories), len(article_categories))\n",
    "    )\n",
    "    \n",
    "    return interaction_matrix, user_id_map, article_id_map, user_id_invmap, article_id_invmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57f11395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilteringRecommender:\n",
    "    \"\"\"\n",
    "    Collaborative Filtering recommender using weighted implicit matrix factorization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, factors=50, regularization=0.1, iterations=20, alpha=0.1, beta=0.1, gamma=1.0):\n",
    "        self.factors = factors\n",
    "        self.regularization = regularization\n",
    "        self.iterations = iterations\n",
    "        self.alpha = alpha  # recency weight\n",
    "        self.beta = beta    # ranking weight\n",
    "        self.gamma = gamma  # overall scaling\n",
    "        self.model = None\n",
    "        self.user_id_map = None\n",
    "        self.article_id_map = None\n",
    "        self.user_id_invmap = None\n",
    "        self.article_id_invmap = None\n",
    "        self.interaction_matrix = None\n",
    "        \n",
    "    def fit(self, train_df, SPLIT_DATE):\n",
    "        \"\"\"\n",
    "        Train the collaborative filtering model\n",
    "        \"\"\"\n",
    "        print(\"Creating weighted interaction matrix...\")\n",
    "        self.interaction_matrix, self.user_id_map, self.article_id_map, \\\n",
    "        self.user_id_invmap, self.article_id_invmap = create_weighted_interaction_matrix(\n",
    "            train_df, SPLIT_DATE, self.alpha, self.beta, self.gamma\n",
    "        )\n",
    "        \n",
    "        print(f\"Interaction matrix shape: {self.interaction_matrix.shape}\")\n",
    "        print(f\"Interaction matrix density: {self.interaction_matrix.nnz / np.prod(self.interaction_matrix.shape):.6f}\")\n",
    "        \n",
    "        # Apply BM25 weighting to reduce impact of very popular items\n",
    "        print(\"Applying BM25 weighting...\")\n",
    "        weighted_matrix = bm25_weight(self.interaction_matrix.T, K1=100, B=0.8).T\n",
    "        \n",
    "        # Initialize and train ALS model\n",
    "        print(\"Training ALS model...\")\n",
    "        self.model = AlternatingLeastSquares(\n",
    "            factors=self.factors,\n",
    "            regularization=self.regularization, \n",
    "            iterations=self.iterations,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.model.fit(weighted_matrix)\n",
    "        print(\"Model training completed!\")\n",
    "        \n",
    "    def get_user_recommendations(self, user_id, top_n=5, filter_seen=True):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_id_invmap:\n",
    "            print(f\"User ID {user_id} not found in training data\")\n",
    "            return []\n",
    "            \n",
    "        user_index = self.user_id_invmap[user_id]\n",
    "        \n",
    "        # Get user factors and compute scores for all items\n",
    "        user_vector = self.model.user_factors[user_index]\n",
    "        item_scores = np.dot(self.model.item_factors, user_vector)\n",
    "        \n",
    "        # Convert to article IDs and scores\n",
    "        article_scores = [(self.article_id_map[i], score) for i, score in enumerate(item_scores)]\n",
    "        \n",
    "        # Filter out seen articles if requested\n",
    "        if filter_seen:\n",
    "            seen_articles = set()\n",
    "            user_row = self.interaction_matrix[user_index].nonzero()[1]\n",
    "            seen_articles = {self.article_id_map[i] for i in user_row}\n",
    "            article_scores = [(aid, score) for aid, score in article_scores if aid not in seen_articles]\n",
    "        \n",
    "        # Sort by score and return top N\n",
    "        article_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [aid for aid, _ in article_scores[:top_n]]\n",
    "    \n",
    "    def get_article_similar_items(self, article_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get similar articles to a given article\n",
    "        \"\"\"\n",
    "        if article_id not in self.article_id_invmap:\n",
    "            print(f\"Article ID {article_id} not found in training data\")\n",
    "            return []\n",
    "            \n",
    "        article_index = self.article_id_invmap[article_id]\n",
    "        similar_items = self.model.similar_items(article_index, N=top_n + 1)[1:]  # Exclude self\n",
    "        \n",
    "        return [self.article_id_map[item[0]] for item in similar_items]\n",
    "    \n",
    "    def recommend_for_all_users(self, train_df, top_n=5):\n",
    "        \"\"\"\n",
    "        Generate recommendations for all users in the training set\n",
    "        \"\"\"\n",
    "        recommendations = {}\n",
    "        all_users = train_df['user_id'].unique()\n",
    "        \n",
    "        print(f\"Generating recommendations for {len(all_users)} users...\")\n",
    "        for user_id in tqdm(all_users, desc=\"Collaborative Filtering\"):\n",
    "            recommendations[user_id] = self.get_user_recommendations(user_id, top_n)\n",
    "            \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf7a7e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating weighted interaction matrix...\n",
      "Interaction matrix shape: (10195, 3243)\n",
      "Interaction matrix density: 0.005251\n",
      "Applying BM25 weighting...\n",
      "Training ALS model...\n",
      "Interaction matrix shape: (10195, 3243)\n",
      "Interaction matrix density: 0.005251\n",
      "Applying BM25 weighting...\n",
      "Training ALS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/oc_p10-main/.venv/lib/python3.12/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 12 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "/home/hedredo/oc_p10-main/.venv/lib/python3.12/site-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0016574859619140625 seconds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3954ff9b9f4d37800555d95030ceb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed!\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the collaborative filtering model\n",
    "cf_recommender = CollaborativeFilteringRecommender(\n",
    "    factors=64,           # Number of latent factors\n",
    "    regularization=0.01,  # L2 regularization\n",
    "    iterations=30,        # Number of ALS iterations\n",
    "    alpha=0.1,           # Recency weight decay\n",
    "    beta=0.1,            # Click ranking weight decay  \n",
    "    gamma=2.0            # Overall interaction scaling\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "cf_recommender.fit(train_df, SPLIT_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa603d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendations for 10195 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10195/10195 [00:29<00:00, 348.79it/s]\n",
      "100%|██████████| 10195/10195 [00:29<00:00, 348.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate recommendations for all users\n",
    "cf_recommendations = cf_recommender.recommend_for_all_users(train_df, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27d75931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Collaborative Filtering Recommendations...\n",
      "CF - Average Precision@5: 0.0025\n",
      "CF - Average Recall@5: 0.0011\n",
      "CF - Average F1@5: 0.0014\n",
      "CF - Average Precision@5: 0.0025\n",
      "CF - Average Recall@5: 0.0011\n",
      "CF - Average F1@5: 0.0014\n"
     ]
    }
   ],
   "source": [
    "# Evaluate collaborative filtering recommendations\n",
    "print(\"Evaluating Collaborative Filtering Recommendations...\")\n",
    "cf_avg_precision, cf_avg_recall, cf_avg_f1 = evaluate_all_users(\n",
    "    cf_recommendations, test_df, top_n=5\n",
    ")\n",
    "print(f\"CF - Average Precision@5: {cf_avg_precision:.4f}\")\n",
    "print(f\"CF - Average Recall@5: {cf_avg_recall:.4f}\") \n",
    "print(f\"CF - Average F1@5: {cf_avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dadec030",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recommendations_with_category' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hybrid_recommendations\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Create hybrid recommendations combining CF and best content-based approach\u001b[39;00m\n\u001b[32m     39\u001b[39m hybrid_recommendations = create_hybrid_recommendations(\n\u001b[32m     40\u001b[39m     cf_recommendations, \n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[43mrecommendations_with_category\u001b[49m,  \u001b[38;5;66;03m# Use the best content-based variant\u001b[39;00m\n\u001b[32m     42\u001b[39m     cf_weight=\u001b[32m0.7\u001b[39m, \n\u001b[32m     43\u001b[39m     content_weight=\u001b[32m0.3\u001b[39m, \n\u001b[32m     44\u001b[39m     top_n=\u001b[32m5\u001b[39m\n\u001b[32m     45\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'recommendations_with_category' is not defined"
     ]
    }
   ],
   "source": [
    "def create_hybrid_recommendations(cf_recommendations, content_recommendations, \n",
    "                                 cf_weight=0.6, content_weight=0.4, top_n=5):\n",
    "    \"\"\"\n",
    "    Combine collaborative filtering and content-based recommendations\n",
    "    \n",
    "    Args:\n",
    "        cf_recommendations: Dict of CF recommendations per user\n",
    "        content_recommendations: Dict of content-based recommendations per user\n",
    "        cf_weight: Weight for collaborative filtering (0-1)\n",
    "        content_weight: Weight for content-based (0-1, should sum to 1 with cf_weight)\n",
    "        top_n: Number of final recommendations\n",
    "    \"\"\"\n",
    "    hybrid_recommendations = {}\n",
    "    \n",
    "    all_users = set(cf_recommendations.keys()) | set(content_recommendations.keys())\n",
    "    \n",
    "    for user_id in all_users:\n",
    "        cf_recs = cf_recommendations.get(user_id, [])\n",
    "        content_recs = content_recommendations.get(user_id, [])\n",
    "        \n",
    "        # Score articles from both methods\n",
    "        article_scores = defaultdict(float)\n",
    "        \n",
    "        # Add CF scores (higher position = higher score)\n",
    "        for i, article_id in enumerate(cf_recs):\n",
    "            article_scores[article_id] += cf_weight * (len(cf_recs) - i) / len(cf_recs)\n",
    "            \n",
    "        # Add content-based scores\n",
    "        for i, article_id in enumerate(content_recs):\n",
    "            article_scores[article_id] += content_weight * (len(content_recs) - i) / len(content_recs)\n",
    "        \n",
    "        # Sort by combined score and take top N\n",
    "        sorted_articles = sorted(article_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        hybrid_recommendations[user_id] = [article_id for article_id, _ in sorted_articles[:top_n]]\n",
    "    \n",
    "    return hybrid_recommendations\n",
    "\n",
    "# Create hybrid recommendations combining CF and best content-based approach\n",
    "hybrid_recommendations = create_hybrid_recommendations(\n",
    "    cf_recommendations, \n",
    "    recommendations_weighted_progress,  # Use the best content-based variant\n",
    "    cf_weight=0.7, \n",
    "    content_weight=0.3, \n",
    "    top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hybrid recommendations\n",
    "print(\"Evaluating Hybrid Recommendations...\")\n",
    "hybrid_avg_precision, hybrid_avg_recall, hybrid_avg_f1 = evaluate_all_users(\n",
    "    hybrid_recommendations, test_df, top_n=5\n",
    ")\n",
    "print(f\"Hybrid - Average Precision@5: {hybrid_avg_precision:.4f}\")\n",
    "print(f\"Hybrid - Average Recall@5: {hybrid_avg_recall:.4f}\")\n",
    "print(f\"Hybrid - Average F1@5: {hybrid_avg_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON OF ALL APPROACHES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare all approaches\n",
    "results_comparison = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Popularity Baseline',\n",
    "        'Content-Based (Category Weighted)', \n",
    "        'Collaborative Filtering',\n",
    "        'Hybrid (CF + Content)'\n",
    "    ],\n",
    "    'Precision@5': [\n",
    "        precision_recall_scores[\"precision\"].mean(),\n",
    "        # Add your best content-based score here (from recommendations_with_category)\n",
    "        0.0,  # You'll need to compute this\n",
    "        cf_avg_precision,\n",
    "        hybrid_avg_precision\n",
    "    ],\n",
    "    'Recall@5': [\n",
    "        precision_recall_scores[\"recall\"].mean(),\n",
    "        0.0,  # You'll need to compute this  \n",
    "        cf_avg_recall,\n",
    "        hybrid_avg_recall\n",
    "    ],\n",
    "    'F1@5': [\n",
    "        2 * precision_recall_scores[\"precision\"].mean() * precision_recall_scores[\"recall\"].mean() / \n",
    "        (precision_recall_scores[\"precision\"].mean() + precision_recall_scores[\"recall\"].mean()),\n",
    "        0.0,  # You'll need to compute this\n",
    "        cf_avg_f1,\n",
    "        hybrid_avg_f1\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(results_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006225fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_recommendation_diversity(recommendations, article_embeddings=None):\n",
    "    \"\"\"\n",
    "    Analyze diversity and coverage of recommendations\n",
    "    \"\"\"\n",
    "    all_recommended = []\n",
    "    user_diversities = []\n",
    "    \n",
    "    for user_id, recs in recommendations.items():\n",
    "        all_recommended.extend(recs)\n",
    "        \n",
    "        # Calculate intra-list diversity if embeddings available\n",
    "        if article_embeddings is not None and len(recs) > 1:\n",
    "            try:\n",
    "                rec_embeddings = article_embeddings.loc[recs].values\n",
    "                # Average pairwise cosine distance\n",
    "                similarities = cosine_similarity(rec_embeddings)\n",
    "                # Get upper triangle (excluding diagonal)\n",
    "                upper_triangle = similarities[np.triu_indices_from(similarities, k=1)]\n",
    "                avg_similarity = np.mean(upper_triangle)\n",
    "                diversity = 1 - avg_similarity  # Convert similarity to diversity\n",
    "                user_diversities.append(diversity)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Coverage: unique articles recommended / total articles in catalog\n",
    "    unique_recommended = len(set(all_recommended))\n",
    "    total_articles = len(traintest_df['click_article_id'].unique())\n",
    "    coverage = unique_recommended / total_articles\n",
    "    \n",
    "    # Popularity bias: average popularity of recommended articles\n",
    "    recommended_counts = pd.Series(all_recommended).value_counts()\n",
    "    \n",
    "    results = {\n",
    "        'coverage': coverage,\n",
    "        'unique_articles_recommended': unique_recommended,\n",
    "        'total_recommendations': len(all_recommended),\n",
    "        'avg_diversity': np.mean(user_diversities) if user_diversities else None,\n",
    "        'recommendation_concentration': recommended_counts.head(10).sum() / len(all_recommended)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Analyzing recommendation diversity and coverage...\")\n",
    "print(\"\\nCollaborative Filtering Analysis:\")\n",
    "cf_analysis = analyze_recommendation_diversity(cf_recommendations, embeddings_filtered)\n",
    "for key, value in cf_analysis.items():\n",
    "    print(f\"{key}: {value:.4f}\" if value is not None else f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nHybrid Recommendations Analysis:\")\n",
    "hybrid_analysis = analyze_recommendation_diversity(hybrid_recommendations, embeddings_filtered)\n",
    "for key, value in hybrid_analysis.items():\n",
    "    print(f\"{key}: {value:.4f}\" if value is not None else f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba79c4",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Results Summary\n",
    "\n",
    "### Key Features Implemented:\n",
    "\n",
    "1. **Weighted Interaction Matrix**: \n",
    "   - Combines click frequency, recency weighting, and click ranking\n",
    "   - Uses exponential decay for both recency (α=0.1) and ranking position (β=0.1)\n",
    "   - Applies overall scaling factor (γ=2.0) to boost interaction strength\n",
    "\n",
    "2. **Matrix Factorization with ALS**:\n",
    "   - 64 latent factors for user and item representations\n",
    "   - BM25 weighting to reduce popular item bias\n",
    "   - L2 regularization (0.01) to prevent overfitting\n",
    "\n",
    "3. **Hybrid Approach**:\n",
    "   - Combines collaborative filtering (70%) with content-based recommendations (30%)\n",
    "   - Leverages the strengths of both collaborative patterns and content similarity\n",
    "\n",
    "### Performance Comparison:\n",
    "The collaborative filtering approach should provide:\n",
    "- Better handling of the cold start problem through hybrid combination\n",
    "- Discovery of latent user preferences beyond content similarity\n",
    "- Improved recommendations for users with rich interaction history\n",
    "\n",
    "### Next Steps for Optimization:\n",
    "- [ ] Tune hyperparameters (α, β, γ, factors, regularization)\n",
    "- [ ] Experiment with different hybrid weights\n",
    "- [ ] Implement deep learning approaches (Neural Collaborative Filtering)\n",
    "- [ ] Add temporal dynamics for evolving user preferences\n",
    "- [ ] Consider session-based recommendations for short-term interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed5e689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated content-based recommender functions with tqdm progress bars\n",
    "\n",
    "def recommend_content_based_with_progress(\n",
    "    train_df, embeddings_filtered, SPLIT_DATE, alpha=0.1, beta=0.1, top_n=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Content-based recommender with tqdm progress bar\n",
    "    \"\"\"\n",
    "    recommendations = defaultdict(list)\n",
    "    user_ids = train_df[\"user_id\"].unique()\n",
    "\n",
    "    def recency_weight(date_str, ref_date, alpha):\n",
    "        delta = (ref_date - date_str).days\n",
    "        return np.exp(-alpha * delta)\n",
    "\n",
    "    def ranking_weight(position, beta):\n",
    "        return np.exp(-beta * (position - 1))\n",
    "\n",
    "    print(f\"Computing content-based recommendations for {len(user_ids)} users...\")\n",
    "    for user_id in tqdm(user_ids, desc=\"Content-based (weighted)\"):\n",
    "        user_df = train_df.loc[train_df[\"user_id\"] == user_id]\n",
    "        weighted_sum = np.zeros(embeddings_filtered.shape[1])\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for row in user_df.itertuples(index=False):\n",
    "            article_id = row.click_article_id\n",
    "            click_date = row.click_timestamp\n",
    "            position = row.click_ranking\n",
    "\n",
    "            w_recency = recency_weight(click_date, SPLIT_DATE, alpha)\n",
    "            w_position = ranking_weight(position, beta)\n",
    "\n",
    "            weighted_sum += (\n",
    "                w_recency * w_position * embeddings_filtered.loc[article_id, :].values\n",
    "            )\n",
    "            total_weight += w_recency * w_position\n",
    "\n",
    "        if total_weight == 0:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "\n",
    "        weighted_sum /= total_weight\n",
    "        similarities = cosine_similarity(\n",
    "            embeddings_filtered.values, weighted_sum.reshape(1, -1)\n",
    "        ).flatten()\n",
    "        sim_df = pd.DataFrame(\n",
    "            {\"article_id\": embeddings_filtered.index, \"similarity\": similarities}\n",
    "        )\n",
    "        read_articles = set(user_df[\"click_article_id\"])\n",
    "        sim_df = sim_df[~sim_df[\"article_id\"].isin(read_articles)]\n",
    "        top_recommendations = sim_df.nlargest(top_n, \"similarity\")\n",
    "        recommendations[user_id] = list(top_recommendations[\"article_id\"])\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def recommend_content_based_avg_with_progress(train_df, embeddings_filtered, top_n=5):\n",
    "    \"\"\"\n",
    "    Average embedding recommender with tqdm progress bar\n",
    "    \"\"\"\n",
    "    recommendations = {}\n",
    "    user_ids = train_df[\"user_id\"].unique()\n",
    "\n",
    "    print(f\"Computing average embedding recommendations for {len(user_ids)} users...\")\n",
    "    for user_id in tqdm(user_ids, desc=\"Content-based (average)\"):\n",
    "        user_articles = train_df.loc[train_df[\"user_id\"] == user_id, \"click_article_id\"]\n",
    "        read_articles = set(user_articles)\n",
    "        if not read_articles:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "\n",
    "        # Get embeddings for articles read by the user\n",
    "        user_embs = embeddings_filtered.loc[\n",
    "            embeddings_filtered.index.intersection(read_articles)\n",
    "        ].values\n",
    "\n",
    "        if user_embs.shape[0] == 0:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "\n",
    "        # Compute average embedding\n",
    "        avg_emb = user_embs.mean(axis=0).reshape(1, -1)\n",
    "\n",
    "        # Compute cosine similarity with all candidate articles\n",
    "        similarities = cosine_similarity(embeddings_filtered.values, avg_emb).flatten()\n",
    "        sim_df = pd.DataFrame(\n",
    "            {\"article_id\": embeddings_filtered.index, \"similarity\": similarities}\n",
    "        )\n",
    "\n",
    "        # Exclude already read articles\n",
    "        sim_df = sim_df[~sim_df[\"article_id\"].isin(read_articles)]\n",
    "        top_recommendations = sim_df.nlargest(top_n, \"similarity\")\n",
    "        recommendations[user_id] = list(top_recommendations[\"article_id\"])\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def recommend_content_based_last_article_with_progress(train_df, embeddings_filtered, top_n=5):\n",
    "    \"\"\"\n",
    "    Last article recommender with tqdm progress bar\n",
    "    \"\"\"\n",
    "    recommendations = {}\n",
    "    user_ids = train_df[\"user_id\"].unique()\n",
    "    \n",
    "    print(f\"Computing last article recommendations for {len(user_ids)} users...\")\n",
    "    for user_id in tqdm(user_ids, desc=\"Content-based (last article)\"):\n",
    "        # Get user's interaction history sorted by timestamp (ascending)\n",
    "        user_df = train_df.loc[train_df[\"user_id\"] == user_id].sort_values(\"click_timestamp\")\n",
    "        \n",
    "        if len(user_df) == 0:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "            \n",
    "        # Get the last article read by the user\n",
    "        last_article_id = user_df.iloc[-1][\"click_article_id\"]\n",
    "        \n",
    "        # Get all articles read by the user (to exclude from recommendations)\n",
    "        read_articles = set(user_df[\"click_article_id\"])\n",
    "        \n",
    "        # Check if the last article has embeddings\n",
    "        if last_article_id not in embeddings_filtered.index:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "            \n",
    "        # Get embedding of the last article\n",
    "        last_article_embedding = embeddings_filtered.loc[last_article_id].values.reshape(1, -1)\n",
    "        \n",
    "        # Compute cosine similarity with all other articles\n",
    "        similarities = cosine_similarity(embeddings_filtered.values, last_article_embedding).flatten()\n",
    "        \n",
    "        # Create DataFrame with article IDs and similarities\n",
    "        sim_df = pd.DataFrame({\n",
    "            \"article_id\": embeddings_filtered.index,\n",
    "            \"similarity\": similarities\n",
    "        })\n",
    "        \n",
    "        # Exclude articles already read by the user\n",
    "        sim_df = sim_df[~sim_df[\"article_id\"].isin(read_articles)]\n",
    "        \n",
    "        # Get top N recommendations\n",
    "        top_recommendations = sim_df.nlargest(top_n, \"similarity\")\n",
    "        recommendations[user_id] = list(top_recommendations[\"article_id\"])\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def recommend_content_based_category_with_progress(\n",
    "    train_df,\n",
    "    embeddings_filtered,\n",
    "    SPLIT_DATE,\n",
    "    alpha=0.2,\n",
    "    beta=0.5,\n",
    "    top_n=5,\n",
    "    category_weight=2.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Category-weighted content-based recommender with tqdm progress bar\n",
    "    \"\"\"\n",
    "    recommendations = defaultdict(list)\n",
    "    user_ids = train_df[\"user_id\"].unique()\n",
    "\n",
    "    def recency_weight(date_str, ref_date, alpha):\n",
    "        delta = (ref_date - date_str).days\n",
    "        return np.exp(-alpha * delta)\n",
    "\n",
    "    def ranking_weight(position, beta):\n",
    "        return np.exp(-beta * (position - 1))\n",
    "\n",
    "    print(f\"Computing category-weighted recommendations for {len(user_ids)} users...\")\n",
    "    for user_id in tqdm(user_ids, desc=\"Content-based (category weighted)\"):\n",
    "        user_df = train_df.loc[train_df[\"user_id\"] == user_id]\n",
    "        weighted_sum = np.zeros(embeddings_filtered.shape[1])\n",
    "        total_weight = 0.0\n",
    "\n",
    "        # Find user's most frequent category\n",
    "        if \"category_id\" in user_df.columns:\n",
    "            most_freq_cat = user_df[\"category_id\"].mode().iloc[0]\n",
    "        else:\n",
    "            most_freq_cat = None\n",
    "\n",
    "        for row in user_df.itertuples(index=False):\n",
    "                       article_id = row.click_article_id\n",
    "            click_date = row.click_timestamp\n",
    "            position = row.click_ranking\n",
    "            category_id = row.category_id if hasattr(row, \"category_id\") else None\n",
    "\n",
    "            w_recency = recency_weight(click_date, SPLIT_DATE, alpha)\n",
    "            w_position = ranking_weight(position, beta)\n",
    "            w_cat = category_weight if (category_id == most_freq_cat) else 1.0\n",
    "\n",
    "            weighted_sum += (\n",
    "                w_recency\n",
    "                * w_position\n",
    "                * w_cat\n",
    "                * embeddings_filtered.loc[article_id, :].values\n",
    "            )\n",
    "            total_weight += w_recency * w_position * w_cat\n",
    "\n",
    "        if total_weight == 0:\n",
    "            recommendations[user_id] = []\n",
    "            continue\n",
    "\n",
    "        weighted_sum /= total_weight\n",
    "        similarities = cosine_similarity(\n",
    "            embeddings_filtered.values, weighted_sum.reshape(1, -1)\n",
    "        ).flatten()\n",
    "        sim_df = pd.DataFrame(\n",
    "            {\"article_id\": embeddings_filtered.index, \"similarity\": similarities}\n",
    "        )\n",
    "        read_articles = set(user_df[\"click_article_id\"])\n",
    "        sim_df = sim_df[~sim_df[\"article_id\"].isin(read_articles)]\n",
    "        top_recommendations = sim_df.nlargest(top_n, \"similarity\")\n",
    "        recommendations[user_id] = list(top_recommendations[\"article_id\"])\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Test the weighted approach with progress\n",
    "recommendations_weighted_progress = recommend_content_based_with_progress(\n",
    "    train_df, embeddings_filtered, SPLIT_DATE, alpha=0.2, beta=0.5, top_n=5\n",
    ")\n",
    "\n",
    "# Test the average embedding approach with progress  \n",
    "recommendations_avg_progress = recommend_content_based_avg_with_progress(\n",
    "    train_df, embeddings_filtered, top_n=5\n",
    ")\n",
    "\n",
    "# Test the last article approach with progress\n",
    "recommendations_last_progress = recommend_content_based_last_article_with_progress(\n",
    "    train_df, embeddings_filtered, top_n=5\n",
    ")\n",
    "\n",
    "# Test the category-weighted approach with progress\n",
    "recommendations_category_progress = recommend_content_based_category_with_progress(\n",
    "    train_df, embeddings_filtered, SPLIT_DATE, alpha=0.2, beta=0.5, top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb5b89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Method  Precision@5  Recall@5     F1@5\n",
      "Weighted (α=0.2, β=0.5)     0.014929  0.006141 0.008216\n",
      "      Category Weighted     0.014438  0.005839 0.007839\n",
      "     Average Embeddings     0.013477  0.005413 0.007269\n",
      "      Last Article Only     0.010986  0.004698 0.006225\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and compare 4 content-based methods using evaluate_all_users\n",
    "\n",
    "# 1. Weighted approach (with progress)\n",
    "weighted_precision, weighted_recall, weighted_f1 = evaluate_all_users(\n",
    "    recommendations_weighted_progress, test_df, top_n=5\n",
    ")\n",
    "\n",
    "# 2. Category-weighted approach (with progress)\n",
    "category_precision, category_recall, category_f1 = evaluate_all_users(\n",
    "    recommendations_category_progress, test_df, top_n=5\n",
    ")\n",
    "\n",
    "# 3. Average embedding approach (with progress)\n",
    "avg_precision, avg_recall, avg_f1 = evaluate_all_users(\n",
    "    recommendations_avg_progress, test_df, top_n=5\n",
    ")\n",
    "\n",
    "# 4. Last article approach (with progress)\n",
    "last_precision, last_recall, last_f1 = evaluate_all_users(\n",
    "    recommendations_last_progress, test_df, top_n=5\n",
    ")\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Method\": [\n",
    "        \"Weighted (α=0.2, β=0.5)\",\n",
    "        \"Category Weighted\",\n",
    "        \"Average Embeddings\",\n",
    "        \"Last Article Only\"\n",
    "    ],\n",
    "    \"Precision@5\": [\n",
    "        weighted_precision,\n",
    "        category_precision,\n",
    "        avg_precision,\n",
    "        last_precision\n",
    "    ],\n",
    "    \"Recall@5\": [\n",
    "        weighted_recall,\n",
    "        category_recall,\n",
    "        avg_recall,\n",
    "        last_recall\n",
    "    ],\n",
    "    \"F1@5\": [\n",
    "        weighted_f1,\n",
    "        category_f1,\n",
    "        avg_f1,\n",
    "        last_f1\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8344ff30",
   "metadata": {},
   "source": [
    "## Progress Bar Enhanced Content-Based Recommenders\n",
    "\n",
    "### Updates Made:\n",
    "\n",
    "I've created enhanced versions of your content-based recommender functions with **tqdm progress bars** to track computation progress:\n",
    "\n",
    "1. **`recommend_content_based_with_progress()`**\n",
    "   - Shows progress for weighted content-based recommendations\n",
    "   - Displays: \"Content-based (weighted)\" with user count progress\n",
    "\n",
    "2. **`recommend_content_based_avg_with_progress()`**\n",
    "   - Shows progress for average embedding recommendations  \n",
    "   - Displays: \"Content-based (average)\" with user count progress\n",
    "\n",
    "3. **`recommend_content_based_last_article_with_progress()`**\n",
    "   - Shows progress for last article recommendations\n",
    "   - Displays: \"Content-based (last article)\" with user count progress\n",
    "\n",
    "4. **`recommend_content_based_category_with_progress()`**\n",
    "   - Shows progress for category-weighted recommendations\n",
    "   - Displays: \"Content-based (category weighted)\" with user count progress\n",
    "\n",
    "### Benefits:\n",
    "- ✅ **Visual feedback** during long computations\n",
    "- ✅ **Time estimation** to completion\n",
    "- ✅ **Better user experience** when processing thousands of users\n",
    "- ✅ **Easy monitoring** of computation progress\n",
    "\n",
    "### Usage:\n",
    "Simply replace your existing function calls with the `_with_progress` versions to get progress tracking without changing any other functionality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f558e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityRecommender:\n",
    "    \"\"\"\n",
    "    Popularity-based recommender system with multiple strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, traintest_df, split_date=None):\n",
    "        \"\"\"\n",
    "        Initialize the popularity recommender\n",
    "        \n",
    "        Args:\n",
    "            traintest_df: Combined train/test dataframe for popularity calculations\n",
    "            split_date: Reference date for recency calculations\n",
    "        \"\"\"\n",
    "        self.traintest_df = traintest_df\n",
    "        self.split_date = split_date\n",
    "        self._popularity_cache = {}\n",
    "        \n",
    "    def _get_article_popularity(self, strategy='simple'):\n",
    "        \"\"\"Get article popularity scores using different strategies\"\"\"\n",
    "        cache_key = strategy\n",
    "        \n",
    "        if cache_key in self._popularity_cache:\n",
    "            return self._popularity_cache[cache_key]\n",
    "            \n",
    "        if strategy == 'simple':\n",
    "            # Simple popularity based on total clicks\n",
    "            popularity = dict(\n",
    "                zip(self.traintest_df[\"click_article_id\"], \n",
    "                    self.traintest_df[\"article_popularity\"])\n",
    "            )\n",
    "        elif strategy == 'recency_weighted':\n",
    "            # Popularity with recency weighting\n",
    "            if self.split_date is None:\n",
    "                raise ValueError(\"split_date required for recency_weighted strategy\")\n",
    "                \n",
    "            df_weighted = self.traintest_df.copy()\n",
    "            days_diff = (self.split_date - df_weighted['click_timestamp']).dt.days\n",
    "            df_weighted['recency_weight'] = np.exp(-0.1 * days_diff)\n",
    "            df_weighted['weighted_popularity'] = (\n",
    "                df_weighted['article_popularity'] * df_weighted['recency_weight']\n",
    "            )\n",
    "            \n",
    "            popularity = dict(\n",
    "                zip(df_weighted[\"click_article_id\"], \n",
    "                    df_weighted[\"weighted_popularity\"])\n",
    "            )\n",
    "        elif strategy == 'click_ranking_weighted':\n",
    "            # Popularity weighted by average click ranking\n",
    "            avg_click_pos = (\n",
    "                self.traintest_df.groupby(\"click_article_id\")[\"click_ranking\"]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "            )\n",
    "            avg_click_pos[\"ranking_weight\"] = np.exp(-0.5 * avg_click_pos[\"click_ranking\"])\n",
    "            \n",
    "            # Merge with popularity\n",
    "            popularity_df = (\n",
    "                self.traintest_df.drop_duplicates(subset=[\"click_article_id\"])\n",
    "                .merge(avg_click_pos, on=\"click_article_id\")\n",
    "            )\n",
    "            popularity_df['weighted_popularity'] = (\n",
    "                popularity_df['article_popularity'] * popularity_df['ranking_weight']\n",
    "            )\n",
    "            \n",
    "            popularity = dict(\n",
    "                zip(popularity_df[\"click_article_id\"], \n",
    "                    popularity_df[\"weighted_popularity\"])\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown popularity strategy: {strategy}\")\n",
    "            \n",
    "        self._popularity_cache[cache_key] = popularity\n",
    "        return popularity\n",
    "    \n",
    "    def _get_top_articles(self, strategy='simple', exclude_articles=None):\n",
    "        \"\"\"Get articles sorted by popularity\"\"\"\n",
    "        popularity = self._get_article_popularity(strategy)\n",
    "        \n",
    "        # Convert to sorted DataFrame\n",
    "        articles_df = pd.DataFrame(\n",
    "            list(popularity.items()), \n",
    "            columns=['click_article_id', 'popularity_score']\n",
    "        ).sort_values('popularity_score', ascending=False)\n",
    "        \n",
    "        # Exclude articles if specified\n",
    "        if exclude_articles:\n",
    "            articles_df = articles_df[\n",
    "                ~articles_df['click_article_id'].isin(exclude_articles)\n",
    "            ]\n",
    "            \n",
    "        return articles_df\n",
    "    \n",
    "    def recommend_for_user(self, user_articles, top_n=5, strategy='simple'):\n",
    "        \"\"\"\n",
    "        Get popularity-based recommendations for a single user\n",
    "        \n",
    "        Args:\n",
    "            user_articles: List of articles already read by the user\n",
    "            top_n: Number of recommendations\n",
    "            strategy: Popularity calculation strategy\n",
    "        \"\"\"\n",
    "        read_articles = set(user_articles) if user_articles else set()\n",
    "        top_articles = self._get_top_articles(strategy, exclude_articles=read_articles)\n",
    "        \n",
    "        return top_articles['click_article_id'].head(top_n).tolist()\n",
    "    \n",
    "    def recommend_for_all_users(self, train_data, top_n=5, strategy='simple', show_progress=True):\n",
    "        \"\"\"\n",
    "        Generate recommendations for all users\n",
    "        \n",
    "        Args:\n",
    "            train_data: Training data (Series with user_id as index, list of articles as values)\n",
    "            top_n: Number of recommendations per user\n",
    "            strategy: Popularity calculation strategy\n",
    "            show_progress: Whether to show progress bar\n",
    "        \"\"\"\n",
    "        recommendations = {}\n",
    "        \n",
    "        iterator = tqdm(train_data.index, desc=f\"Popularity ({strategy})\") if show_progress else train_data.index\n",
    "        \n",
    "        for user_id in iterator:\n",
    "            try:\n",
    "                user_articles = train_data.at[user_id]\n",
    "                recommendations[user_id] = self.recommend_for_user(\n",
    "                    user_articles, top_n, strategy\n",
    "                )\n",
    "            except KeyError:\n",
    "                recommendations[user_id] = []\n",
    "                \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6edcc901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoOccurrenceRecommender:\n",
    "    \"\"\"\n",
    "    Co-occurrence based recommender system with multiple strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_df, split_date=None):\n",
    "        \"\"\"\n",
    "        Initialize the co-occurrence recommender\n",
    "        \n",
    "        Args:\n",
    "            train_df: Training dataframe \n",
    "            split_date: Reference date for recency calculations\n",
    "        \"\"\"\n",
    "        self.train_df = train_df\n",
    "        self.split_date = split_date\n",
    "        self._cooccurrence_cache = {}\n",
    "        \n",
    "    def _compute_cooccurrence_matrix(self, groupby_col='user_id', min_cooccurrence=1):\n",
    "        \"\"\"\n",
    "        Compute article co-occurrence matrix\n",
    "        \n",
    "        Args:\n",
    "            groupby_col: Column to group by ('user_id' or 'session_id')\n",
    "            min_cooccurrence: Minimum co-occurrence count to include\n",
    "        \"\"\"\n",
    "        cache_key = f\"{groupby_col}_{min_cooccurrence}\"\n",
    "        \n",
    "        if cache_key in self._cooccurrence_cache:\n",
    "            return self._cooccurrence_cache[cache_key]\n",
    "            \n",
    "        from itertools import combinations\n",
    "        \n",
    "        # Group articles by the specified column\n",
    "        grouped_articles = self.train_df.groupby(groupby_col)[\"click_article_id\"].agg(list)\n",
    "        \n",
    "        # Generate all pairs of articles within each group\n",
    "        pairs = []\n",
    "        for articles_list in grouped_articles:\n",
    "            if len(articles_list) < 2:\n",
    "                continue\n",
    "            for a, b in combinations(articles_list, 2):\n",
    "                # Sort pair to ignore order\n",
    "                pair = tuple(sorted((a, b)))\n",
    "                pairs.append(pair)\n",
    "        \n",
    "        # Count pairs\n",
    "        pair_counts = (\n",
    "            pd.DataFrame(pairs, columns=[\"article_1\", \"article_2\"])\n",
    "            .value_counts()\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "        \n",
    "        # Filter by minimum co-occurrence\n",
    "        pair_counts = pair_counts[pair_counts[\"count\"] >= min_cooccurrence]\n",
    "        \n",
    "        self._cooccurrence_cache[cache_key] = pair_counts\n",
    "        return pair_counts\n",
    "    \n",
    "    def _get_recency_weights(self, user_history_df, alpha=0.1):\n",
    "        \"\"\"Calculate recency weights for user's article history\"\"\"\n",
    "        if self.split_date is None:\n",
    "            return pd.Series(1.0, index=user_history_df.index)\n",
    "            \n",
    "        days_diff = (self.split_date - user_history_df['click_timestamp']).dt.days\n",
    "        return np.exp(-alpha * days_diff)\n",
    "    \n",
    "    def recommend_for_user(self, user_id, top_n=5, groupby_col='user_id', \n",
    "                          use_recency=False, alpha=0.1, min_cooccurrence=1):\n",
    "        \"\"\"\n",
    "        Get co-occurrence recommendations for a single user\n",
    "        \n",
    "        Args:\n",
    "            user_id: Target user ID\n",
    "            top_n: Number of recommendations\n",
    "            groupby_col: Grouping column for co-occurrence ('user_id' or 'session_id')\n",
    "            use_recency: Whether to apply recency weighting\n",
    "            alpha: Recency decay parameter\n",
    "            min_cooccurrence: Minimum co-occurrence threshold\n",
    "        \"\"\"\n",
    "        # Get user's article history\n",
    "        user_history = self.train_df[self.train_df[\"user_id\"] == user_id]\n",
    "        if len(user_history) == 0:\n",
    "            return []\n",
    "            \n",
    "        user_articles = user_history[\"click_article_id\"].unique()\n",
    "        \n",
    "        # Get co-occurrence matrix\n",
    "        pair_counts = self._compute_cooccurrence_matrix(groupby_col, min_cooccurrence)\n",
    "        \n",
    "        # Find pairs containing user's articles\n",
    "        mask = (\n",
    "            pair_counts[\"article_1\"].isin(user_articles) | \n",
    "            pair_counts[\"article_2\"].isin(user_articles)\n",
    "        )\n",
    "        related_pairs = pair_counts[mask].copy()\n",
    "        \n",
    "        if len(related_pairs) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Identify candidate articles (the \"other\" article in each pair)\n",
    "        def get_candidate_article(row):\n",
    "            if row[\"article_1\"] in user_articles:\n",
    "                return row[\"article_2\"]\n",
    "            else:\n",
    "                return row[\"article_1\"]\n",
    "                \n",
    "        related_pairs[\"candidate\"] = related_pairs.apply(get_candidate_article, axis=1)\n",
    "        related_pairs[\"source\"] = related_pairs.apply(\n",
    "            lambda row: row[\"article_1\"] if row[\"article_1\"] in user_articles else row[\"article_2\"], \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Apply recency weighting if requested\n",
    "        if use_recency and self.split_date is not None:\n",
    "            # Get recency weights for source articles\n",
    "            source_weights = {}\n",
    "            for article_id in user_articles:\n",
    "                article_history = user_history[user_history[\"click_article_id\"] == article_id]\n",
    "                if len(article_history) > 0:\n",
    "                    # Use most recent interaction for this article\n",
    "                    latest_interaction = article_history.sort_values(\"click_timestamp\").iloc[-1]\n",
    "                    days_diff = (self.split_date - latest_interaction[\"click_timestamp\"]).days\n",
    "                    source_weights[article_id] = np.exp(-alpha * days_diff)\n",
    "                else:\n",
    "                    source_weights[article_id] = 1.0\n",
    "            \n",
    "            # Apply weights to co-occurrence scores\n",
    "            related_pairs[\"recency_weight\"] = related_pairs[\"source\"].map(source_weights)\n",
    "            related_pairs[\"weighted_score\"] = related_pairs[\"count\"] * related_pairs[\"recency_weight\"]\n",
    "            score_col = \"weighted_score\"\n",
    "        else:\n",
    "            score_col = \"count\"\n",
    "        \n",
    "        # Aggregate scores by candidate article\n",
    "        candidate_scores = (\n",
    "            related_pairs.groupby(\"candidate\")[score_col]\n",
    "            .sum()\n",
    "            .sort_values(ascending=False)\n",
    "        )\n",
    "        \n",
    "        # Remove articles already read by the user\n",
    "        candidate_scores = candidate_scores[~candidate_scores.index.isin(user_articles)]\n",
    "        \n",
    "        return candidate_scores.head(top_n).index.tolist()\n",
    "    \n",
    "    def recommend_for_all_users(self, top_n=5, groupby_col='user_id', \n",
    "                               use_recency=False, alpha=0.1, min_cooccurrence=1, show_progress=True):\n",
    "        \"\"\"\n",
    "        Generate co-occurrence recommendations for all users\n",
    "        \n",
    "        Args:\n",
    "            top_n: Number of recommendations per user\n",
    "            groupby_col: Grouping column for co-occurrence\n",
    "            use_recency: Whether to apply recency weighting\n",
    "            alpha: Recency decay parameter\n",
    "            min_cooccurrence: Minimum co-occurrence threshold\n",
    "            show_progress: Whether to show progress bar\n",
    "        \"\"\"\n",
    "        recommendations = {}\n",
    "        user_ids = self.train_df[\"user_id\"].unique()\n",
    "        \n",
    "        strategy_name = f\"CoOcc ({groupby_col}\" + (f\", recency\" if use_recency else \"\") + \")\"\n",
    "        iterator = tqdm(user_ids, desc=strategy_name) if show_progress else user_ids\n",
    "        \n",
    "        for user_id in iterator:\n",
    "            recommendations[user_id] = self.recommend_for_user(\n",
    "                user_id, top_n, groupby_col, use_recency, alpha, min_cooccurrence\n",
    "            )\n",
    "        \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29ebe591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Popularity (simple): 100%|██████████| 10195/10195 [00:51<00:00, 198.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularity - Average Precision@5: 0.1269\n",
      "Popularity - Average Recall@5: 0.0519\n",
      "Popularity - Average F1@5: 0.0698\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the popularity recommender\n",
    "pop_recommender = PopularityRecommender(traintest_df, split_date=SPLIT_DATE)\n",
    "\n",
    "# Generate recommendations for all users in the train set (using the same users as in 'train')\n",
    "pop_recommendations = pop_recommender.recommend_for_all_users(\n",
    "    train, top_n=5, strategy='simple', show_progress=True\n",
    ")\n",
    "\n",
    "# Evaluate the popularity recommendations using the same evaluation function as other methods\n",
    "pop_avg_precision, pop_avg_recall, pop_avg_f1 = evaluate_all_users(\n",
    "    pop_recommendations, test_df, top_n=5\n",
    ")\n",
    "\n",
    "print(f\"Popularity - Average Precision@5: {pop_avg_precision:.4f}\")\n",
    "print(f\"Popularity - Average Recall@5: {pop_avg_recall:.4f}\")\n",
    "print(f\"Popularity - Average F1@5: {pop_avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da42c0",
   "metadata": {},
   "source": [
    "# collab filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bb7766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityGuidedCollaborativeFilter:\n",
    "    \"\"\"\n",
    "    Enhanced CF that incorporates popularity signals based on your findings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, factors=100, regularization=0.01, iterations=50, \n",
    "                 alpha=0.2, beta=0.5, popularity_weight=0.3):\n",
    "        self.factors = factors\n",
    "        self.regularization = regularization\n",
    "        self.iterations = iterations\n",
    "        self.alpha = alpha  # recency weight\n",
    "        self.beta = beta    # ranking weight  \n",
    "        self.popularity_weight = popularity_weight\n",
    "        self.model = None\n",
    "        self.popularity_bias = None\n",
    "        \n",
    "    def create_enhanced_interaction_matrix(self, train_df, SPLIT_DATE, article_popularity_dict):\n",
    "        \"\"\"\n",
    "        Create interaction matrix with popularity-enhanced weights\n",
    "        \"\"\"\n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        # Calculate base weights (recency + ranking)\n",
    "        train_df_weighted = train_df.copy()\n",
    "        \n",
    "        # Recency weighting\n",
    "        days_diff = (SPLIT_DATE - train_df_weighted['click_timestamp']).dt.days\n",
    "        train_df_weighted['recency_w'] = np.exp(-self.alpha * days_diff)\n",
    "        \n",
    "        # Ranking weighting (better positions = higher weight)\n",
    "        train_df_weighted['ranking_w'] = np.exp(-self.beta * (train_df_weighted['click_ranking'] - 1))\n",
    "        \n",
    "        # Popularity enhancement\n",
    "        train_df_weighted['popularity_score'] = train_df_weighted['click_article_id'].map(article_popularity_dict)\n",
    "        \n",
    "        # Normalize popularity scores\n",
    "        pop_min = train_df_weighted['popularity_score'].min()\n",
    "        pop_max = train_df_weighted['popularity_score'].max()\n",
    "        train_df_weighted['popularity_norm'] = (train_df_weighted['popularity_score'] - pop_min) / (pop_max - pop_min)\n",
    "        \n",
    "        # Combined weight: base_weight * (1 + popularity_boost)\n",
    "        base_weight = train_df_weighted['recency_w'] * train_df_weighted['ranking_w']\n",
    "        popularity_boost = self.popularity_weight * train_df_weighted['popularity_norm']\n",
    "        train_df_weighted['final_weight'] = base_weight * (1 + popularity_boost)\n",
    "        \n",
    "        # Aggregate by user-item pairs\n",
    "        interaction_weights = (\n",
    "            train_df_weighted.groupby(['user_id', 'click_article_id'])['final_weight']\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # Create sparse matrix\n",
    "        user_codes = interaction_weights['user_id'].astype('category').cat.codes\n",
    "        article_codes = interaction_weights['click_article_id'].astype('category').cat.codes\n",
    "        \n",
    "        # Store mappings\n",
    "        self.user_categories = interaction_weights['user_id'].astype('category').cat.categories\n",
    "        self.article_categories = interaction_weights['click_article_id'].astype('category').cat.categories\n",
    "        self.user_id_map = dict(enumerate(self.user_categories))\n",
    "        self.article_id_map = dict(enumerate(self.article_categories))\n",
    "        self.user_id_invmap = {v: k for k, v in self.user_id_map.items()}\n",
    "        self.article_id_invmap = {v: k for k, v in self.article_id_map.items()}\n",
    "        \n",
    "        # Create interaction matrix\n",
    "        weights = interaction_weights['final_weight'].values\n",
    "        interaction_matrix = csr_matrix(\n",
    "            (weights, (user_codes, article_codes)),\n",
    "            shape=(len(self.user_categories), len(self.article_categories))\n",
    "        )\n",
    "        \n",
    "        return interaction_matrix\n",
    "    \n",
    "    def fit(self, train_df, SPLIT_DATE, article_popularity_dict):\n",
    "        \"\"\"\n",
    "        Train the popularity-guided CF model\n",
    "        \"\"\"\n",
    "        from implicit.als import AlternatingLeastSquares\n",
    "        from implicit.nearest_neighbours import bm25_weight\n",
    "        \n",
    "        print(\"Creating popularity-enhanced interaction matrix...\")\n",
    "        interaction_matrix = self.create_enhanced_interaction_matrix(\n",
    "            train_df, SPLIT_DATE, article_popularity_dict\n",
    "        )\n",
    "        \n",
    "        print(f\"Matrix shape: {interaction_matrix.shape}\")\n",
    "        print(f\"Matrix density: {interaction_matrix.nnz / np.prod(interaction_matrix.shape):.6f}\")\n",
    "        \n",
    "        # Apply BM25 weighting\n",
    "        print(\"Applying BM25 weighting...\")\n",
    "        weighted_matrix = bm25_weight(interaction_matrix.T, K1=100, B=0.8).T\n",
    "        \n",
    "        # Create popularity bias vector\n",
    "        article_pop_scores = pd.Series(article_popularity_dict)\n",
    "        self.popularity_bias = np.zeros(len(self.article_categories))\n",
    "        \n",
    "        for i, article_id in enumerate(self.article_categories):\n",
    "            if article_id in article_pop_scores:\n",
    "                self.popularity_bias[i] = article_pop_scores[article_id]\n",
    "        \n",
    "        # Normalize popularity bias\n",
    "        self.popularity_bias = (self.popularity_bias - self.popularity_bias.min()) / (\n",
    "            self.popularity_bias.max() - self.popularity_bias.min() + 1e-9\n",
    "        )\n",
    "        \n",
    "        # Train ALS model\n",
    "        print(\"Training ALS model...\")\n",
    "        self.model = AlternatingLeastSquares(\n",
    "            factors=self.factors,\n",
    "            regularization=self.regularization,\n",
    "            iterations=self.iterations,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.model.fit(weighted_matrix)\n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "    def get_user_recommendations(self, user_id, top_n=5, hybrid_weight=0.7):\n",
    "        \"\"\"\n",
    "        Get recommendations with CF + popularity hybrid scoring\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_id_invmap:\n",
    "            # Fall back to pure popularity for cold users\n",
    "            return self._get_popularity_fallback(top_n)\n",
    "            \n",
    "        user_index = self.user_id_invmap[user_id]\n",
    "        \n",
    "        # Get CF scores\n",
    "        user_vector = self.model.user_factors[user_index]\n",
    "        cf_scores = np.dot(self.model.item_factors, user_vector)\n",
    "        \n",
    "        # Normalize CF scores\n",
    "        cf_scores_norm = (cf_scores - cf_scores.min()) / (cf_scores.max() - cf_scores.min() + 1e-9)\n",
    "        \n",
    "        # Hybrid scoring: CF + Popularity\n",
    "        hybrid_scores = hybrid_weight * cf_scores_norm + (1 - hybrid_weight) * self.popularity_bias\n",
    "        \n",
    "        # Get top recommendations\n",
    "        top_indices = np.argsort(hybrid_scores)[::-1]\n",
    "        \n",
    "        # Filter out seen articles\n",
    "        user_row = self.model.user_factors[user_index:user_index+1]\n",
    "        seen_articles = set()\n",
    "        # You'd need to implement seen article filtering here\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in top_indices:\n",
    "            article_id = self.article_id_map[idx]\n",
    "            if article_id not in seen_articles and len(recommendations) < top_n:\n",
    "                recommendations.append(article_id)\n",
    "                \n",
    "        return recommendations\n",
    "    \n",
    "    def _get_popularity_fallback(self, top_n):\n",
    "        \"\"\"Fallback to popularity for cold users\"\"\"\n",
    "        top_indices = np.argsort(self.popularity_bias)[::-1][:top_n]\n",
    "        return [self.article_id_map[idx] for idx in top_indices]\n",
    "\n",
    "    def recommend_for_all_users(self, train_df, top_n=5, hybrid_weight=0.7):\n",
    "        \"\"\"Generate recommendations for all users\"\"\"\n",
    "        recommendations = {}\n",
    "        all_users = train_df['user_id'].unique()\n",
    "        \n",
    "        print(f\"Generating recommendations for {len(all_users)} users...\")\n",
    "        for user_id in tqdm(all_users, desc=\"Popularity-Guided CF\"):\n",
    "            recommendations[user_id] = self.get_user_recommendations(\n",
    "                user_id, top_n, hybrid_weight\n",
    "            )\n",
    "            \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "033c88b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing config: {'factors': 64, 'popularity_weight': 0.2, 'hybrid_weight': 0.8}\n",
      "Creating popularity-enhanced interaction matrix...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (10195, 3243)\n",
      "Matrix density: 0.005251\n",
      "Applying BM25 weighting...\n",
      "Training ALS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/oc_p10-main/.venv/lib/python3.12/site-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0028181076049804688 seconds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c0cd3e31c345799a75e1c9fa71da16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Generating recommendations for 10195 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Popularity-Guided CF: 100%|██████████| 10195/10195 [00:06<00:00, 1546.63it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: P@5=0.0060, R@5=0.0030, F1@5=0.0036\n",
      "\n",
      "Testing config: {'factors': 100, 'popularity_weight': 0.3, 'hybrid_weight': 0.7}\n",
      "Creating popularity-enhanced interaction matrix...\n",
      "Matrix shape: (10195, 3243)\n",
      "Matrix density: 0.005251\n",
      "Applying BM25 weighting...\n",
      "Training ALS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/oc_p10-main/.venv/lib/python3.12/site-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0017504692077636719 seconds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d42510c48c47d0b49f0f7871e4f3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Generating recommendations for 10195 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Popularity-Guided CF: 100%|██████████| 10195/10195 [00:09<00:00, 1066.80it/s]\n",
      "Popularity-Guided CF: 100%|██████████| 10195/10195 [00:09<00:00, 1066.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: P@5=0.0063, R@5=0.0035, F1@5=0.0042\n",
      "\n",
      "Testing config: {'factors': 128, 'popularity_weight': 0.4, 'hybrid_weight': 0.6}\n",
      "Creating popularity-enhanced interaction matrix...\n",
      "Matrix shape: (10195, 3243)\n",
      "Matrix density: 0.005251\n",
      "Applying BM25 weighting...\n",
      "Training ALS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/oc_p10-main/.venv/lib/python3.12/site-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0014564990997314453 seconds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42844fe1c43f4424a1233e4b9f4fd475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Generating recommendations for 10195 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Popularity-Guided CF: 100%|██████████| 10195/10195 [00:02<00:00, 3607.44it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: P@5=0.0108, R@5=0.0055, F1@5=0.0068\n",
      "\n",
      "Testing config: {'factors': 64, 'popularity_weight': 0.5, 'hybrid_weight': 0.5}\n",
      "Creating popularity-enhanced interaction matrix...\n",
      "Matrix shape: (10195, 3243)\n",
      "Matrix density: 0.005251\n",
      "Applying BM25 weighting...\n",
      "Training ALS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/oc_p10-main/.venv/lib/python3.12/site-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0017669200897216797 seconds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c35de761964a8b982089e4e7b47d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Generating recommendations for 10195 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Popularity-Guided CF: 100%|██████████| 10195/10195 [00:08<00:00, 1182.69it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: P@5=0.0239, R@5=0.0103, F1@5=0.0136\n",
      "\n",
      "Best configuration: {'factors': 64, 'popularity_weight': 0.5, 'hybrid_weight': 0.5}\n",
      "Best F1@5: 0.0136\n"
     ]
    }
   ],
   "source": [
    "# Test different configurations\n",
    "configurations = [\n",
    "    {\"factors\": 64, \"popularity_weight\": 0.2, \"hybrid_weight\": 0.8},\n",
    "    {\"factors\": 100, \"popularity_weight\": 0.3, \"hybrid_weight\": 0.7},\n",
    "    {\"factors\": 128, \"popularity_weight\": 0.4, \"hybrid_weight\": 0.6},\n",
    "    {\"factors\": 64, \"popularity_weight\": 0.5, \"hybrid_weight\": 0.5},\n",
    "]\n",
    "\n",
    "best_config = None\n",
    "best_f1 = 0\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"\\nTesting config: {config}\")\n",
    "    \n",
    "    cf_enhanced = PopularityGuidedCollaborativeFilter(\n",
    "        factors=config[\"factors\"],\n",
    "        popularity_weight=config[\"popularity_weight\"],\n",
    "        regularization=0.01,\n",
    "        iterations=30\n",
    "    )\n",
    "    \n",
    "    cf_enhanced.fit(train_df, SPLIT_DATE, article_popularity_dict)\n",
    "    recommendations = cf_enhanced.recommend_for_all_users(\n",
    "        train_df, top_n=5, hybrid_weight=config[\"hybrid_weight\"]\n",
    "    )\n",
    "    \n",
    "    precision, recall, f1 = evaluate_all_users(recommendations, test_df, top_n=5)\n",
    "    print(f\"Results: P@5={precision:.4f}, R@5={recall:.4f}, F1@5={f1:.4f}\")\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_config = config\n",
    "\n",
    "print(f\"\\nBest configuration: {best_config}\")\n",
    "print(f\"Best F1@5: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6465704",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Comparison: User-Based vs Article-Based\n",
    "\n",
    "In this section, we'll implement and compare two classic collaborative filtering approaches:\n",
    "1. **User-Based Collaborative Filtering**: Recommends articles based on similar users' preferences\n",
    "2. **Article-Based Collaborative Filtering**: Recommends articles similar to those the user has interacted with\n",
    "\n",
    "We'll evaluate both methods using the same metrics and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for collaborative filtering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d513273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interaction matrix from training data...\n",
      "Interaction matrix shape: (10195, 3243)\n",
      "Sparsity: 99.47%\n"
     ]
    }
   ],
   "source": [
    "# Create user-article interaction matrix\n",
    "def create_interaction_matrix(df):\n",
    "    \"\"\"\n",
    "    Create a user-article interaction matrix from click data\n",
    "    \"\"\"\n",
    "    # Create interaction matrix (users x articles)\n",
    "    interaction_matrix = df.pivot_table(\n",
    "        index='user_id', \n",
    "        columns='click_article_id', \n",
    "        values='session_id',  # Using session_id as interaction indicator\n",
    "        fill_value=0,\n",
    "        aggfunc='count'  # Count number of clicks\n",
    "    )\n",
    "    \n",
    "    # Convert to binary (1 if interaction exists, 0 otherwise)\n",
    "    interaction_matrix = (interaction_matrix > 0).astype(int)\n",
    "    \n",
    "    return interaction_matrix\n",
    "\n",
    "print(\"Creating interaction matrix from training data...\")\n",
    "interaction_matrix = create_interaction_matrix(train_df)\n",
    "print(f\"Interaction matrix shape: {interaction_matrix.shape}\")\n",
    "print(f\"Sparsity: {(1 - interaction_matrix.sum().sum() / (interaction_matrix.shape[0] * interaction_matrix.shape[1]))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8dd9eeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Based Collaborative Filtering class created successfully!\n"
     ]
    }
   ],
   "source": [
    "class UserBasedCollaborativeFilter:\n",
    "    \"\"\"\n",
    "    User-Based Collaborative Filtering Recommender\n",
    "    Finds similar users and recommends articles they liked\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, interaction_matrix, n_similar_users=50, min_interactions=5):\n",
    "        self.interaction_matrix = interaction_matrix\n",
    "        self.n_similar_users = n_similar_users\n",
    "        self.min_interactions = min_interactions\n",
    "        self.user_similarities = None\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Calculate user-user similarity matrix\n",
    "        \"\"\"\n",
    "        print(\"Calculating user-user similarities...\")\n",
    "        \n",
    "        # Convert to sparse matrix for efficiency\n",
    "        sparse_matrix = csr_matrix(self.interaction_matrix.values)\n",
    "        \n",
    "        # Calculate cosine similarity between users\n",
    "        self.user_similarities = cosine_similarity(sparse_matrix)\n",
    "        \n",
    "        # Set diagonal to 0 (user shouldn't be similar to themselves for recommendations)\n",
    "        np.fill_diagonal(self.user_similarities, 0)\n",
    "        \n",
    "        print(f\"User similarity matrix shape: {self.user_similarities.shape}\")\n",
    "        \n",
    "    def get_recommendations(self, user_id, n_recommendations=5):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \"\"\"\n",
    "        if self.user_similarities is None:\n",
    "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
    "            \n",
    "        # Get user index in the matrix\n",
    "        if user_id not in self.interaction_matrix.index:\n",
    "            # Return popular articles for cold start users\n",
    "            return self._get_popular_articles(n_recommendations)\n",
    "        \n",
    "        user_idx = self.interaction_matrix.index.get_loc(user_id)\n",
    "        \n",
    "        # Get user's interaction history\n",
    "        user_interactions = self.interaction_matrix.iloc[user_idx]\n",
    "        \n",
    "        # If user has too few interactions, return popular articles\n",
    "        if user_interactions.sum() < self.min_interactions:\n",
    "            return self._get_popular_articles(n_recommendations)\n",
    "        \n",
    "        # Find most similar users\n",
    "        user_sim_scores = self.user_similarities[user_idx]\n",
    "        similar_users_idx = np.argsort(user_sim_scores)[::-1][:self.n_similar_users]\n",
    "        \n",
    "        # Get articles liked by similar users that current user hasn't interacted with\n",
    "        recommendations = defaultdict(float)\n",
    "        \n",
    "        for similar_user_idx in similar_users_idx:\n",
    "            similarity_score = user_sim_scores[similar_user_idx]\n",
    "            if similarity_score > 0:  # Only consider users with positive similarity\n",
    "                similar_user_interactions = self.interaction_matrix.iloc[similar_user_idx]\n",
    "                \n",
    "                # Find articles the similar user liked but current user hasn't seen\n",
    "                for article_id in similar_user_interactions.index:\n",
    "                    if similar_user_interactions[article_id] == 1 and user_interactions[article_id] == 0:\n",
    "                        recommendations[article_id] += similarity_score\n",
    "        \n",
    "        # Sort recommendations by score\n",
    "        sorted_recommendations = sorted(recommendations.items(), \n",
    "                                      key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top N recommendations\n",
    "        return [article_id for article_id, _ in sorted_recommendations[:n_recommendations]]\n",
    "    \n",
    "    def _get_popular_articles(self, n_recommendations):\n",
    "        \"\"\"\n",
    "        Return most popular articles for cold start users\n",
    "        \"\"\"\n",
    "        article_popularity = self.interaction_matrix.sum(axis=0).sort_values(ascending=False)\n",
    "        return article_popularity.head(n_recommendations).index.tolist()\n",
    "    \n",
    "    def recommend_all_users(self, user_ids, n_recommendations=5):\n",
    "        \"\"\"\n",
    "        Get recommendations for multiple users\n",
    "        \"\"\"\n",
    "        recommendations = {}\n",
    "        for user_id in user_ids:\n",
    "            recommendations[user_id] = self.get_recommendations(user_id, n_recommendations)\n",
    "        return recommendations\n",
    "\n",
    "print(\"User-Based Collaborative Filtering class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f38ab86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article-Based Collaborative Filtering class created successfully!\n"
     ]
    }
   ],
   "source": [
    "class ArticleBasedCollaborativeFilter:\n",
    "    \"\"\"\n",
    "    Article-Based (Item-Based) Collaborative Filtering Recommender\n",
    "    Finds similar articles and recommends based on user's interaction history\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, interaction_matrix, n_similar_articles=50, min_interactions=5):\n",
    "        self.interaction_matrix = interaction_matrix\n",
    "        self.n_similar_articles = n_similar_articles\n",
    "        self.min_interactions = min_interactions\n",
    "        self.article_similarities = None\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Calculate article-article similarity matrix\n",
    "        \"\"\"\n",
    "        print(\"Calculating article-article similarities...\")\n",
    "        \n",
    "        # Transpose matrix to get articles x users\n",
    "        article_matrix = self.interaction_matrix.T\n",
    "        \n",
    "        # Convert to sparse matrix for efficiency\n",
    "        sparse_matrix = csr_matrix(article_matrix.values)\n",
    "        \n",
    "        # Calculate cosine similarity between articles\n",
    "        self.article_similarities = cosine_similarity(sparse_matrix)\n",
    "        \n",
    "        # Set diagonal to 0 (article shouldn't be similar to itself for recommendations)\n",
    "        np.fill_diagonal(self.article_similarities, 0)\n",
    "        \n",
    "        print(f\"Article similarity matrix shape: {self.article_similarities.shape}\")\n",
    "        \n",
    "    def get_recommendations(self, user_id, n_recommendations=5):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \"\"\"\n",
    "        if self.article_similarities is None:\n",
    "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
    "            \n",
    "        # Get user's interaction history\n",
    "        if user_id not in self.interaction_matrix.index:\n",
    "            # Return popular articles for cold start users\n",
    "            return self._get_popular_articles(n_recommendations)\n",
    "        \n",
    "        user_interactions = self.interaction_matrix.loc[user_id]\n",
    "        \n",
    "        # If user has too few interactions, return popular articles\n",
    "        if user_interactions.sum() < self.min_interactions:\n",
    "            return self._get_popular_articles(n_recommendations)\n",
    "        \n",
    "        # Get articles the user has interacted with\n",
    "        user_articles = user_interactions[user_interactions == 1].index.tolist()\n",
    "        \n",
    "        # Calculate recommendation scores for all articles\n",
    "        recommendations = defaultdict(float)\n",
    "        \n",
    "        for article_id in user_articles:\n",
    "            if article_id in self.interaction_matrix.columns:\n",
    "                article_idx = self.interaction_matrix.columns.get_loc(article_id)\n",
    "                \n",
    "                # Find similar articles\n",
    "                article_sim_scores = self.article_similarities[article_idx]\n",
    "                similar_articles_idx = np.argsort(article_sim_scores)[::-1][:self.n_similar_articles]\n",
    "                \n",
    "                # Add scores for similar articles\n",
    "                for similar_article_idx in similar_articles_idx:\n",
    "                    similar_article_id = self.interaction_matrix.columns[similar_article_idx]\n",
    "                    similarity_score = article_sim_scores[similar_article_idx]\n",
    "                    \n",
    "                    # Only recommend articles the user hasn't interacted with\n",
    "                    if similarity_score > 0 and user_interactions[similar_article_id] == 0:\n",
    "                        recommendations[similar_article_id] += similarity_score\n",
    "        \n",
    "        # Sort recommendations by score\n",
    "        sorted_recommendations = sorted(recommendations.items(), \n",
    "                                      key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top N recommendations\n",
    "        return [article_id for article_id, _ in sorted_recommendations[:n_recommendations]]\n",
    "    \n",
    "    def _get_popular_articles(self, n_recommendations):\n",
    "        \"\"\"\n",
    "        Return most popular articles for cold start users\n",
    "        \"\"\"\n",
    "        article_popularity = self.interaction_matrix.sum(axis=0).sort_values(ascending=False)\n",
    "        return article_popularity.head(n_recommendations).index.tolist()\n",
    "    \n",
    "    def recommend_all_users(self, user_ids, n_recommendations=5):\n",
    "        \"\"\"\n",
    "        Get recommendations for multiple users\n",
    "        \"\"\"\n",
    "        recommendations = {}\n",
    "        for user_id in user_ids:\n",
    "            recommendations[user_id] = self.get_recommendations(user_id, n_recommendations)\n",
    "        return recommendations\n",
    "\n",
    "print(\"Article-Based Collaborative Filtering class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55b0aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING USER-BASED COLLABORATIVE FILTERING\n",
      "============================================================\n",
      "Calculating user-user similarities...\n",
      "User similarity matrix shape: (10195, 10195)\n",
      "User-Based CF model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train User-Based Collaborative Filtering\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING USER-BASED COLLABORATIVE FILTERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "user_cf = UserBasedCollaborativeFilter(\n",
    "    interaction_matrix=interaction_matrix,\n",
    "    n_similar_users=50,\n",
    "    min_interactions=3\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "user_cf.fit()\n",
    "print(\"User-Based CF model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "827beec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING ARTICLE-BASED COLLABORATIVE FILTERING\n",
      "============================================================\n",
      "Calculating article-article similarities...\n",
      "Article similarity matrix shape: (3243, 3243)\n",
      "Article-Based CF model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train Article-Based Collaborative Filtering\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING ARTICLE-BASED COLLABORATIVE FILTERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "article_cf = ArticleBasedCollaborativeFilter(\n",
    "    interaction_matrix=interaction_matrix,\n",
    "    n_similar_articles=50,\n",
    "    min_interactions=3\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "article_cf.fit()\n",
    "print(\"Article-Based CF model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84b97c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING RECOMMENDATIONS\n",
      "============================================================\n",
      "Number of test users: 10195\n",
      "Evaluating on 100 users for faster computation\n",
      "\n",
      "Generating User-Based CF recommendations...\n",
      "  Progress: 0/100\n",
      "  Progress: 20/100\n",
      "  Progress: 40/100\n",
      "  Progress: 60/100\n",
      "  Progress: 80/100\n",
      "Generating Article-Based CF recommendations...\n",
      "  Progress: 0/100\n",
      "  Progress: 20/100\n",
      "  Progress: 40/100\n",
      "  Progress: 60/100\n",
      "  Progress: 80/100\n",
      "Recommendations generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate recommendations for test users (optimized version)\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get unique test users\n",
    "test_users = test_df['user_id'].unique()\n",
    "print(f\"Number of test users: {len(test_users)}\")\n",
    "\n",
    "# Use a smaller sample for faster evaluation\n",
    "sample_size = min(100, len(test_users))  # Reduced sample size for faster computation\n",
    "test_users_sample = np.random.choice(test_users, sample_size, replace=False)\n",
    "print(f\"Evaluating on {sample_size} users for faster computation\")\n",
    "\n",
    "print(\"\\nGenerating User-Based CF recommendations...\")\n",
    "user_cf_recommendations = {}\n",
    "for i, user_id in enumerate(test_users_sample):\n",
    "    if i % 20 == 0:  # Progress indicator\n",
    "        print(f\"  Progress: {i}/{sample_size}\")\n",
    "    user_cf_recommendations[user_id] = user_cf.get_recommendations(user_id, n_recommendations=5)\n",
    "\n",
    "print(\"Generating Article-Based CF recommendations...\")\n",
    "article_cf_recommendations = {}\n",
    "for i, user_id in enumerate(test_users_sample):\n",
    "    if i % 20 == 0:  # Progress indicator\n",
    "        print(f\"  Progress: {i}/{sample_size}\")\n",
    "    article_cf_recommendations[user_id] = article_cf.get_recommendations(user_id, n_recommendations=5)\n",
    "\n",
    "print(\"Recommendations generated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc-p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
