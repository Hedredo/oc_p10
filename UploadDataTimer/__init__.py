import logging
import os
import tempfile
import requests
import zipfile
import pandas as pd
from azure.storage.blob import BlobServiceClient
import azure.functions as func

# Pour la compression PCA
from sklearn.decomposition import PCA
import pickle

# Param√®tres √† personnaliser
ZIP_URL = "https://ton-url-source/ton-fichier.zip"
CONTAINER_NAME = "nom-du-conteneur"
CSV_FILENAME = "ton_fichier.csv"         # Nom du CSV √† extraire du ZIP
PARQUET_FILENAME = "ton_fichier.parquet" # Nom du fichier Parquet √† uploader
EMBEDDINGS_FILENAME = "articles_embeddings.pickle"  # Nom du pickle √† uploader
EMBEDDINGS_BLOB_NAME = "articles_embeddings_compressed.pickle"  # Nom sur le blob
N_COMPONENTS_PCA = 100  # √Ä ajuster selon ton besoin

def ensure_container_exists(blob_service_client, container_name):
    try:
        container_client = blob_service_client.get_container_client(container_name)
        container_client.get_container_properties()
        logging.info(f"‚úÖ Le conteneur '{container_name}' existe d√©j√†.")
    except Exception:
        logging.info(f"üîπ Le conteneur '{container_name}' n'existe pas, cr√©ation en cours...")
        blob_service_client.create_container(container_name)
        logging.info(f"‚úÖ Conteneur '{container_name}' cr√©√© avec succ√®s.")

def download_zip(url, dest_path):
    try:
        logging.info(f"üîπ T√©l√©chargement de {url} ...")
        response = requests.get(url, stream=True)
        response.raise_for_status()
        with open(dest_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        logging.info("‚úÖ T√©l√©chargement termin√© !")
    except Exception as e:
        logging.error(f"‚ùå Erreur lors du t√©l√©chargement : {e}")
        raise

def extract_zip(zip_path, extract_to):
    try:
        logging.info(f"üîπ Extraction de {zip_path} ...")
        with zipfile.ZipFile(zip_path, "r") as zip_ref:
            zip_ref.extractall(extract_to)
        logging.info("‚úÖ Extraction termin√©e !")
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de l'extraction : {e}")
        raise

def convert_csv_to_parquet(csv_path, parquet_path):
    try:
        logging.info(f"üîπ Conversion {csv_path} ‚Üí {parquet_path} ...")
        df = pd.read_csv(csv_path)
        df.to_parquet(parquet_path)
        logging.info("‚úÖ Conversion termin√©e !")
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la conversion CSV ‚Üí Parquet : {e}")
        raise

def compress_embeddings_with_pca(pickle_path, compressed_pickle_path, n_components=100):
    try:
        logging.info(f"üîπ Compression PCA de {pickle_path} ...")
        with open(pickle_path, "rb") as f:
            embeddings = pickle.load(f)
        # Si embeddings est un DataFrame ou ndarray
        pca = PCA(n_components=n_components)
        embeddings_reduced = pca.fit_transform(embeddings)
        with open(compressed_pickle_path, "wb") as f:
            pickle.dump(embeddings_reduced, f)
        logging.info(f"‚úÖ Compression PCA termin√©e ({n_components} composantes).")
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la compression PCA : {e}")
        raise

def upload_to_blob(blob_service_client, container_name, file_path, blob_name):
    try:
        logging.info(f"üîπ Upload de {file_path} vers Azure Blob Storage ({container_name}/{blob_name}) ...")
        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
        with open(file_path, "rb") as data:
            blob_client.upload_blob(data, overwrite=True)
        logging.info("‚úÖ Upload termin√© !")
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de l'upload sur Azure Blob Storage : {e}")
        raise

def main(mytimer: func.TimerRequest) -> None:
    logging.info("=== D√©but de la t√¢che planifi√©e UploadDataTimer ===")
    try:
        # R√©cup√©rer la cha√Æne de connexion depuis les variables d'environnement Azure
        BLOB_CONNECTION_STRING = os.environ["AzureWebJobsStorage"]
        blob_service_client = BlobServiceClient.from_connection_string(BLOB_CONNECTION_STRING)

        # V√©rifier/cr√©er le conteneur
        ensure_container_exists(blob_service_client, CONTAINER_NAME)

        with tempfile.TemporaryDirectory() as tmpdir:
            zip_path = os.path.join(tmpdir, "data.zip")
            download_zip(ZIP_URL, zip_path)
            extract_zip(zip_path, tmpdir)

            # CSV ‚Üí Parquet
            csv_path = os.path.join(tmpdir, CSV_FILENAME)
            parquet_path = os.path.join(tmpdir, PARQUET_FILENAME)
            if not os.path.exists(csv_path):
                logging.error(f"‚ùå Fichier CSV {CSV_FILENAME} introuvable apr√®s extraction.")
            else:
                convert_csv_to_parquet(csv_path, parquet_path)
                upload_to_blob(blob_service_client, CONTAINER_NAME, parquet_path, PARQUET_FILENAME)

            # Embeddings pickle ‚Üí PCA compress√© ‚Üí upload
            embeddings_path = os.path.join(tmpdir, EMBEDDINGS_FILENAME)
            compressed_embeddings_path = os.path.join(tmpdir, EMBEDDINGS_BLOB_NAME)
            if not os.path.exists(embeddings_path):
                logging.error(f"‚ùå Fichier embeddings {EMBEDDINGS_FILENAME} introuvable apr√®s extraction.")
            else:
                compress_embeddings_with_pca(embeddings_path, compressed_embeddings_path, N_COMPONENTS_PCA)
                upload_to_blob(blob_service_client, CONTAINER_NAME, compressed_embeddings_path, EMBEDDINGS_BLOB_NAME)

        logging.info("=== T√¢che UploadDataTimer termin√©e avec succ√®s ===")
    except Exception as e:
        logging.error(f"‚ùå Erreur dans la fonction UploadDataTimer : {e}")